{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8e19cae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:01.697661Z",
     "iopub.status.busy": "2023-06-24T16:57:01.697310Z",
     "iopub.status.idle": "2023-06-24T16:57:01.704531Z",
     "shell.execute_reply": "2023-06-24T16:57:01.703038Z",
     "shell.execute_reply.started": "2023-06-24T16:57:01.697637Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Energy_Models.Transformer import Transformer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6efc8bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:01.808914Z",
     "iopub.status.busy": "2023-06-24T16:57:01.808428Z",
     "iopub.status.idle": "2023-06-24T16:57:01.919773Z",
     "shell.execute_reply": "2023-06-24T16:57:01.919100Z",
     "shell.execute_reply.started": "2023-06-24T16:57:01.808888Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/appliances-energy-prediction/KAG_energydata_complete.csv',parse_dates=['date'], index_col= 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ce4d024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:01.921490Z",
     "iopub.status.busy": "2023-06-24T16:57:01.921108Z",
     "iopub.status.idle": "2023-06-24T16:57:01.929894Z",
     "shell.execute_reply": "2023-06-24T16:57:01.928933Z",
     "shell.execute_reply.started": "2023-06-24T16:57:01.921469Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.resample('D').sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "350e7112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:01.931376Z",
     "iopub.status.busy": "2023-06-24T16:57:01.931071Z",
     "iopub.status.idle": "2023-06-24T16:57:01.946836Z",
     "shell.execute_reply": "2023-06-24T16:57:01.945518Z",
     "shell.execute_reply.started": "2023-06-24T16:57:01.931331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 28)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resample the data on daily basis\n",
    "df = df.resample('D').mean()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae7b0828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:01.949449Z",
     "iopub.status.busy": "2023-06-24T16:57:01.949097Z",
     "iopub.status.idle": "2023-06-24T16:57:01.978611Z",
     "shell.execute_reply": "2023-06-24T16:57:01.977369Z",
     "shell.execute_reply.started": "2023-06-24T16:57:01.949426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-11</th>\n",
       "      <td>136.666667</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.808571</td>\n",
       "      <td>46.906210</td>\n",
       "      <td>20.153333</td>\n",
       "      <td>44.793175</td>\n",
       "      <td>20.140972</td>\n",
       "      <td>45.992440</td>\n",
       "      <td>19.492222</td>\n",
       "      <td>47.260813</td>\n",
       "      <td>...</td>\n",
       "      <td>16.918175</td>\n",
       "      <td>45.106210</td>\n",
       "      <td>5.922619</td>\n",
       "      <td>735.173810</td>\n",
       "      <td>89.285714</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>40.845238</td>\n",
       "      <td>4.228571</td>\n",
       "      <td>26.248800</td>\n",
       "      <td>26.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-12</th>\n",
       "      <td>85.694444</td>\n",
       "      <td>4.236111</td>\n",
       "      <td>20.092326</td>\n",
       "      <td>45.142014</td>\n",
       "      <td>19.307655</td>\n",
       "      <td>43.779733</td>\n",
       "      <td>19.993646</td>\n",
       "      <td>44.933171</td>\n",
       "      <td>19.772778</td>\n",
       "      <td>44.556840</td>\n",
       "      <td>...</td>\n",
       "      <td>17.032847</td>\n",
       "      <td>45.680127</td>\n",
       "      <td>5.566319</td>\n",
       "      <td>742.628125</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>5.784722</td>\n",
       "      <td>37.229167</td>\n",
       "      <td>3.596528</td>\n",
       "      <td>25.505072</td>\n",
       "      <td>25.505072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-13</th>\n",
       "      <td>97.013889</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>19.197824</td>\n",
       "      <td>42.867772</td>\n",
       "      <td>18.568513</td>\n",
       "      <td>42.119669</td>\n",
       "      <td>19.609213</td>\n",
       "      <td>43.720891</td>\n",
       "      <td>19.009803</td>\n",
       "      <td>42.315174</td>\n",
       "      <td>...</td>\n",
       "      <td>16.953576</td>\n",
       "      <td>44.691476</td>\n",
       "      <td>4.856944</td>\n",
       "      <td>754.790625</td>\n",
       "      <td>83.406250</td>\n",
       "      <td>5.614583</td>\n",
       "      <td>31.559028</td>\n",
       "      <td>2.169792</td>\n",
       "      <td>23.464726</td>\n",
       "      <td>23.464726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-14</th>\n",
       "      <td>151.388889</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>20.372078</td>\n",
       "      <td>42.435191</td>\n",
       "      <td>19.743131</td>\n",
       "      <td>40.699172</td>\n",
       "      <td>20.837581</td>\n",
       "      <td>43.399740</td>\n",
       "      <td>18.694271</td>\n",
       "      <td>43.231053</td>\n",
       "      <td>...</td>\n",
       "      <td>16.884815</td>\n",
       "      <td>45.428229</td>\n",
       "      <td>3.431944</td>\n",
       "      <td>749.767361</td>\n",
       "      <td>86.420139</td>\n",
       "      <td>6.277778</td>\n",
       "      <td>35.149306</td>\n",
       "      <td>1.316667</td>\n",
       "      <td>25.264569</td>\n",
       "      <td>25.264569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-15</th>\n",
       "      <td>125.347222</td>\n",
       "      <td>5.972222</td>\n",
       "      <td>22.280949</td>\n",
       "      <td>39.099294</td>\n",
       "      <td>21.528172</td>\n",
       "      <td>38.193897</td>\n",
       "      <td>20.959074</td>\n",
       "      <td>41.409884</td>\n",
       "      <td>19.615127</td>\n",
       "      <td>41.961036</td>\n",
       "      <td>...</td>\n",
       "      <td>17.345694</td>\n",
       "      <td>44.534329</td>\n",
       "      <td>2.665278</td>\n",
       "      <td>754.579861</td>\n",
       "      <td>88.385417</td>\n",
       "      <td>7.770833</td>\n",
       "      <td>40.208333</td>\n",
       "      <td>0.849653</td>\n",
       "      <td>26.289515</td>\n",
       "      <td>26.289515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-23</th>\n",
       "      <td>83.055556</td>\n",
       "      <td>2.013889</td>\n",
       "      <td>24.445046</td>\n",
       "      <td>47.025307</td>\n",
       "      <td>22.825978</td>\n",
       "      <td>47.749664</td>\n",
       "      <td>25.801574</td>\n",
       "      <td>42.977436</td>\n",
       "      <td>23.765955</td>\n",
       "      <td>45.311262</td>\n",
       "      <td>...</td>\n",
       "      <td>23.096240</td>\n",
       "      <td>46.394452</td>\n",
       "      <td>12.570486</td>\n",
       "      <td>755.849653</td>\n",
       "      <td>84.225694</td>\n",
       "      <td>4.208333</td>\n",
       "      <td>46.017361</td>\n",
       "      <td>9.851736</td>\n",
       "      <td>24.611743</td>\n",
       "      <td>24.611743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24</th>\n",
       "      <td>96.180556</td>\n",
       "      <td>4.236111</td>\n",
       "      <td>24.299340</td>\n",
       "      <td>42.112535</td>\n",
       "      <td>22.617992</td>\n",
       "      <td>42.677685</td>\n",
       "      <td>25.368941</td>\n",
       "      <td>39.967807</td>\n",
       "      <td>23.869067</td>\n",
       "      <td>41.552216</td>\n",
       "      <td>...</td>\n",
       "      <td>22.765058</td>\n",
       "      <td>42.042955</td>\n",
       "      <td>11.568750</td>\n",
       "      <td>759.007292</td>\n",
       "      <td>76.680556</td>\n",
       "      <td>4.114583</td>\n",
       "      <td>33.975694</td>\n",
       "      <td>7.469097</td>\n",
       "      <td>25.494683</td>\n",
       "      <td>25.494683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-25</th>\n",
       "      <td>83.888889</td>\n",
       "      <td>2.569444</td>\n",
       "      <td>23.899353</td>\n",
       "      <td>38.929815</td>\n",
       "      <td>22.894673</td>\n",
       "      <td>38.182158</td>\n",
       "      <td>25.038329</td>\n",
       "      <td>38.137895</td>\n",
       "      <td>23.874266</td>\n",
       "      <td>38.582405</td>\n",
       "      <td>...</td>\n",
       "      <td>22.147823</td>\n",
       "      <td>39.681940</td>\n",
       "      <td>11.563542</td>\n",
       "      <td>756.887500</td>\n",
       "      <td>74.774306</td>\n",
       "      <td>1.350694</td>\n",
       "      <td>30.739583</td>\n",
       "      <td>6.819097</td>\n",
       "      <td>27.630188</td>\n",
       "      <td>27.630188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-26</th>\n",
       "      <td>147.013889</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>24.055271</td>\n",
       "      <td>42.417511</td>\n",
       "      <td>24.557538</td>\n",
       "      <td>38.836517</td>\n",
       "      <td>26.034081</td>\n",
       "      <td>38.775326</td>\n",
       "      <td>24.188176</td>\n",
       "      <td>40.728358</td>\n",
       "      <td>...</td>\n",
       "      <td>22.439097</td>\n",
       "      <td>43.400563</td>\n",
       "      <td>14.923611</td>\n",
       "      <td>756.619792</td>\n",
       "      <td>74.336806</td>\n",
       "      <td>1.850694</td>\n",
       "      <td>39.329861</td>\n",
       "      <td>9.636458</td>\n",
       "      <td>22.590179</td>\n",
       "      <td>22.590179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27</th>\n",
       "      <td>136.330275</td>\n",
       "      <td>0.642202</td>\n",
       "      <td>24.456330</td>\n",
       "      <td>46.208005</td>\n",
       "      <td>24.322729</td>\n",
       "      <td>43.420283</td>\n",
       "      <td>27.087699</td>\n",
       "      <td>40.811682</td>\n",
       "      <td>24.531437</td>\n",
       "      <td>43.707752</td>\n",
       "      <td>...</td>\n",
       "      <td>22.906277</td>\n",
       "      <td>46.232320</td>\n",
       "      <td>17.195413</td>\n",
       "      <td>755.784862</td>\n",
       "      <td>74.275229</td>\n",
       "      <td>2.041284</td>\n",
       "      <td>46.596330</td>\n",
       "      <td>12.246330</td>\n",
       "      <td>25.332458</td>\n",
       "      <td>25.332458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Appliances     lights         T1       RH_1         T2       RH_2  \\\n",
       "date                                                                            \n",
       "2016-01-11  136.666667  30.000000  20.808571  46.906210  20.153333  44.793175   \n",
       "2016-01-12   85.694444   4.236111  20.092326  45.142014  19.307655  43.779733   \n",
       "2016-01-13   97.013889   5.416667  19.197824  42.867772  18.568513  42.119669   \n",
       "2016-01-14  151.388889   5.000000  20.372078  42.435191  19.743131  40.699172   \n",
       "2016-01-15  125.347222   5.972222  22.280949  39.099294  21.528172  38.193897   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "2016-05-23   83.055556   2.013889  24.445046  47.025307  22.825978  47.749664   \n",
       "2016-05-24   96.180556   4.236111  24.299340  42.112535  22.617992  42.677685   \n",
       "2016-05-25   83.888889   2.569444  23.899353  38.929815  22.894673  38.182158   \n",
       "2016-05-26  147.013889   3.125000  24.055271  42.417511  24.557538  38.836517   \n",
       "2016-05-27  136.330275   0.642202  24.456330  46.208005  24.322729  43.420283   \n",
       "\n",
       "                   T3       RH_3         T4       RH_4  ...         T9  \\\n",
       "date                                                    ...              \n",
       "2016-01-11  20.140972  45.992440  19.492222  47.260813  ...  16.918175   \n",
       "2016-01-12  19.993646  44.933171  19.772778  44.556840  ...  17.032847   \n",
       "2016-01-13  19.609213  43.720891  19.009803  42.315174  ...  16.953576   \n",
       "2016-01-14  20.837581  43.399740  18.694271  43.231053  ...  16.884815   \n",
       "2016-01-15  20.959074  41.409884  19.615127  41.961036  ...  17.345694   \n",
       "...               ...        ...        ...        ...  ...        ...   \n",
       "2016-05-23  25.801574  42.977436  23.765955  45.311262  ...  23.096240   \n",
       "2016-05-24  25.368941  39.967807  23.869067  41.552216  ...  22.765058   \n",
       "2016-05-25  25.038329  38.137895  23.874266  38.582405  ...  22.147823   \n",
       "2016-05-26  26.034081  38.775326  24.188176  40.728358  ...  22.439097   \n",
       "2016-05-27  27.087699  40.811682  24.531437  43.707752  ...  22.906277   \n",
       "\n",
       "                 RH_9      T_out  Press_mm_hg     RH_out  Windspeed  \\\n",
       "date                                                                  \n",
       "2016-01-11  45.106210   5.922619   735.173810  89.285714   6.083333   \n",
       "2016-01-12  45.680127   5.566319   742.628125  87.500000   5.784722   \n",
       "2016-01-13  44.691476   4.856944   754.790625  83.406250   5.614583   \n",
       "2016-01-14  45.428229   3.431944   749.767361  86.420139   6.277778   \n",
       "2016-01-15  44.534329   2.665278   754.579861  88.385417   7.770833   \n",
       "...               ...        ...          ...        ...        ...   \n",
       "2016-05-23  46.394452  12.570486   755.849653  84.225694   4.208333   \n",
       "2016-05-24  42.042955  11.568750   759.007292  76.680556   4.114583   \n",
       "2016-05-25  39.681940  11.563542   756.887500  74.774306   1.350694   \n",
       "2016-05-26  43.400563  14.923611   756.619792  74.336806   1.850694   \n",
       "2016-05-27  46.232320  17.195413   755.784862  74.275229   2.041284   \n",
       "\n",
       "            Visibility  Tdewpoint        rv1        rv2  \n",
       "date                                                     \n",
       "2016-01-11   40.845238   4.228571  26.248800  26.248800  \n",
       "2016-01-12   37.229167   3.596528  25.505072  25.505072  \n",
       "2016-01-13   31.559028   2.169792  23.464726  23.464726  \n",
       "2016-01-14   35.149306   1.316667  25.264569  25.264569  \n",
       "2016-01-15   40.208333   0.849653  26.289515  26.289515  \n",
       "...                ...        ...        ...        ...  \n",
       "2016-05-23   46.017361   9.851736  24.611743  24.611743  \n",
       "2016-05-24   33.975694   7.469097  25.494683  25.494683  \n",
       "2016-05-25   30.739583   6.819097  27.630188  27.630188  \n",
       "2016-05-26   39.329861   9.636458  22.590179  22.590179  \n",
       "2016-05-27   46.596330  12.246330  25.332458  25.332458  \n",
       "\n",
       "[138 rows x 28 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d43aec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:01.980900Z",
     "iopub.status.busy": "2023-06-24T16:57:01.980412Z",
     "iopub.status.idle": "2023-06-24T16:57:01.987627Z",
     "shell.execute_reply": "2023-06-24T16:57:01.986427Z",
     "shell.execute_reply.started": "2023-06-24T16:57:01.980867Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    \n",
    "    # compute split point\n",
    "    end_idx = df.shape[0]* 70 // 100\n",
    "    \n",
    "    train_data = df.iloc[:end_idx, : ]\n",
    "    test_data = df.iloc[end_idx:, :]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb685760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:01.989566Z",
     "iopub.status.busy": "2023-06-24T16:57:01.988771Z",
     "iopub.status.idle": "2023-06-24T16:57:02.000591Z",
     "shell.execute_reply": "2023-06-24T16:57:01.999157Z",
     "shell.execute_reply.started": "2023-06-24T16:57:01.989540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71d3e713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:02.002010Z",
     "iopub.status.busy": "2023-06-24T16:57:02.001739Z",
     "iopub.status.idle": "2023-06-24T16:57:02.012720Z",
     "shell.execute_reply": "2023-06-24T16:57:02.011990Z",
     "shell.execute_reply.started": "2023-06-24T16:57:02.001987Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_data(train, test):\n",
    "    scaler = MinMaxScaler().fit(train)\n",
    "    return scaler.transform(train), scaler.transform(test), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49a4b393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:02.018092Z",
     "iopub.status.busy": "2023-06-24T16:57:02.016697Z",
     "iopub.status.idle": "2023-06-24T16:57:02.031723Z",
     "shell.execute_reply": "2023-06-24T16:57:02.030772Z",
     "shell.execute_reply.started": "2023-06-24T16:57:02.018049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "X_train, X_test, scaler = scale_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82d4ddb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:02.034547Z",
     "iopub.status.busy": "2023-06-24T16:57:02.033657Z",
     "iopub.status.idle": "2023-06-24T16:57:02.040925Z",
     "shell.execute_reply": "2023-06-24T16:57:02.040158Z",
     "shell.execute_reply.started": "2023-06-24T16:57:02.034517Z"
    }
   },
   "outputs": [],
   "source": [
    "def multivariate_splitter(df):\n",
    "\n",
    "    input_features = []\n",
    "    ouput_feature = []\n",
    "    \n",
    "    len_df = df.shape[0]\n",
    "    \n",
    "    for i in range(len_df):\n",
    "        \n",
    "        end_idx = i + 15\n",
    "        \n",
    "        if end_idx > len_df-15:\n",
    "            break\n",
    "            \n",
    "        input_x , output_y = df[i:end_idx, 1:], df[end_idx: end_idx+15, 0]\n",
    "        \n",
    "        input_features.append(input_x)\n",
    "        ouput_feature.append(output_y)\n",
    "    \n",
    "    return np.array(input_features), np.array(ouput_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "855a50d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:02.042283Z",
     "iopub.status.busy": "2023-06-24T16:57:02.041976Z",
     "iopub.status.idle": "2023-06-24T16:57:02.059528Z",
     "shell.execute_reply": "2023-06-24T16:57:02.058451Z",
     "shell.execute_reply.started": "2023-06-24T16:57:02.042223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of (training data) input features : (67, 15, 27) and ouput feature (67, 15)\n",
      "Shape of (testing data) input features : (13, 15, 27) and ouput feature (13, 15)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = multivariate_splitter(X_train)\n",
    "X_test, Y_test = multivariate_splitter(X_test)\n",
    "print('Shape of (training data) input features : %s and ouput feature %s' % (X_train.shape, Y_train.shape))\n",
    "print('Shape of (testing data) input features : %s and ouput feature %s' % (X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e16b9081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:02.061149Z",
     "iopub.status.busy": "2023-06-24T16:57:02.060737Z",
     "iopub.status.idle": "2023-06-24T16:57:02.072099Z",
     "shell.execute_reply": "2023-06-24T16:57:02.071083Z",
     "shell.execute_reply.started": "2023-06-24T16:57:02.061126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67, 15, 27), (67, 15))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f0167b08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:02.073953Z",
     "iopub.status.busy": "2023-06-24T16:57:02.073294Z",
     "iopub.status.idle": "2023-06-24T16:57:03.510985Z",
     "shell.execute_reply": "2023-06-24T16:57:03.509544Z",
     "shell.execute_reply.started": "2023-06-24T16:57:02.073927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 15, 27)]     0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_60 (LayerN  (None, 15, 27)      54          ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_30 (Multi  (None, 15, 27)      1803        ['layer_normalization_60[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_63 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_30[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_60 (TFOpL  (None, 15, 27)      0           ['dropout_63[0][0]',             \n",
      " ambda)                                                           'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_61 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_60[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_60 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_64 (Dropout)           (None, 15, 32)       0           ['conv1d_60[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_61 (Conv1D)             (None, 15, 27)       891         ['dropout_64[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_61 (TFOpL  (None, 15, 27)      0           ['conv1d_61[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_60[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_62 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_61[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_31 (Multi  (None, 15, 27)      1803        ['layer_normalization_62[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_31[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_62 (TFOpL  (None, 15, 27)      0           ['dropout_65[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_61[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_63 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_62[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_62 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)           (None, 15, 32)       0           ['conv1d_62[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_63 (Conv1D)             (None, 15, 27)       891         ['dropout_66[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_63 (TFOpL  (None, 15, 27)      0           ['conv1d_63[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_62[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_64 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_63[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_32 (Multi  (None, 15, 27)      1803        ['layer_normalization_64[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_32[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_64 (TFOpL  (None, 15, 27)      0           ['dropout_67[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_63[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_65 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_64[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_64 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)           (None, 15, 32)       0           ['conv1d_64[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_65 (Conv1D)             (None, 15, 27)       891         ['dropout_68[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_65 (TFOpL  (None, 15, 27)      0           ['conv1d_65[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_64[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_66 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_65[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_33 (Multi  (None, 15, 27)      1803        ['layer_normalization_66[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_33[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_66 (TFOpL  (None, 15, 27)      0           ['dropout_69[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_65[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_67 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_66[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_66 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)           (None, 15, 32)       0           ['conv1d_66[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_67 (Conv1D)             (None, 15, 27)       891         ['dropout_70[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_67 (TFOpL  (None, 15, 27)      0           ['conv1d_67[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_66[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_68 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_67[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_34 (Multi  (None, 15, 27)      1803        ['layer_normalization_68[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_34[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_68 (TFOpL  (None, 15, 27)      0           ['dropout_71[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_67[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_69 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_68[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_68 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_72 (Dropout)           (None, 15, 32)       0           ['conv1d_68[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_69 (Conv1D)             (None, 15, 27)       891         ['dropout_72[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_69 (TFOpL  (None, 15, 27)      0           ['conv1d_69[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_68[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_70 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_69[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_35 (Multi  (None, 15, 27)      1803        ['layer_normalization_70[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_35[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_70 (TFOpL  (None, 15, 27)      0           ['dropout_73[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_69[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_71 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_70[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_70 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 15, 32)       0           ['conv1d_70[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_71 (Conv1D)             (None, 15, 27)       891         ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_71 (TFOpL  (None, 15, 27)      0           ['conv1d_71[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_70[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_72 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_71[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_36 (Multi  (None, 15, 27)      1803        ['layer_normalization_72[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_36[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_72 (TFOpL  (None, 15, 27)      0           ['dropout_75[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_71[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_73 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_72[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_72 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)           (None, 15, 32)       0           ['conv1d_72[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_73 (Conv1D)             (None, 15, 27)       891         ['dropout_76[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_73 (TFOpL  (None, 15, 27)      0           ['conv1d_73[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_72[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_74 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_73[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_37 (Multi  (None, 15, 27)      1803        ['layer_normalization_74[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_77 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_37[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_74 (TFOpL  (None, 15, 27)      0           ['dropout_77[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_73[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_75 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_74[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_74 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_78 (Dropout)           (None, 15, 32)       0           ['conv1d_74[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_75 (Conv1D)             (None, 15, 27)       891         ['dropout_78[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_75 (TFOpL  (None, 15, 27)      0           ['conv1d_75[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_74[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_76 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_75[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_38 (Multi  (None, 15, 27)      1803        ['layer_normalization_76[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_38[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_76 (TFOpL  (None, 15, 27)      0           ['dropout_79[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_75[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_77 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_76[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_76 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)           (None, 15, 32)       0           ['conv1d_76[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_77 (Conv1D)             (None, 15, 27)       891         ['dropout_80[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_77 (TFOpL  (None, 15, 27)      0           ['conv1d_77[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_76[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_78 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_77[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_39 (Multi  (None, 15, 27)      1803        ['layer_normalization_78[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_81 (Dropout)           (None, 15, 27)       0           ['multi_head_attention_39[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_78 (TFOpL  (None, 15, 27)      0           ['dropout_81[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_77[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_79 (LayerN  (None, 15, 27)      54          ['tf.__operators__.add_78[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_78 (Conv1D)             (None, 15, 32)       896         ['layer_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_82 (Dropout)           (None, 15, 32)       0           ['conv1d_78[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_79 (Conv1D)             (None, 15, 27)       891         ['dropout_82[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_79 (TFOpL  (None, 15, 27)      0           ['conv1d_79[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_78[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 27)          0           ['tf.__operators__.add_79[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          14336       ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_83 (Dropout)           (None, 512)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 15)           7695        ['dropout_83[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 59,011\n",
      "Trainable params: 59,011\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "model = Transformer(    \n",
    "    input_shape,\n",
    "    n_outputs=15,\n",
    "    head_size=4,\n",
    "    num_heads=4,\n",
    "    ff_dim=32,\n",
    "    num_transformer_blocks=10,\n",
    "    mlp_units=[512],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.2,).build_model()\n",
    "\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dea7f56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:03.516154Z",
     "iopub.status.busy": "2023-06-24T16:57:03.515158Z",
     "iopub.status.idle": "2023-06-24T16:57:25.216802Z",
     "shell.execute_reply": "2023-06-24T16:57:25.215515Z",
     "shell.execute_reply.started": "2023-06-24T16:57:03.516119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 15s 1s/step - loss: 0.8586 - val_loss: 0.2031\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2938 - val_loss: 0.1556\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2416 - val_loss: 0.1784\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1718 - val_loss: 0.1628\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1385 - val_loss: 0.1369\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1198 - val_loss: 0.1163\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.1102 - val_loss: 0.1002\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0963 - val_loss: 0.0893\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0905 - val_loss: 0.0868\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0776 - val_loss: 0.0863\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0707 - val_loss: 0.0858\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0734 - val_loss: 0.0919\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0708 - val_loss: 0.0952\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0614 - val_loss: 0.0884\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0601 - val_loss: 0.0822\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0584 - val_loss: 0.0799\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0582 - val_loss: 0.0814\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0519 - val_loss: 0.0860\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0578 - val_loss: 0.0865\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0543 - val_loss: 0.0839\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0532 - val_loss: 0.0796\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0498 - val_loss: 0.0758\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0533 - val_loss: 0.0729\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0516 - val_loss: 0.0719\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0533 - val_loss: 0.0729\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0482 - val_loss: 0.0750\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0465 - val_loss: 0.0775\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0487 - val_loss: 0.0780\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0475 - val_loss: 0.0753\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0510 - val_loss: 0.0719\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0448 - val_loss: 0.0695\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0437 - val_loss: 0.0688\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0475 - val_loss: 0.0682\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0453 - val_loss: 0.0669\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0493 - val_loss: 0.0659\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0434 - val_loss: 0.0661\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0430 - val_loss: 0.0672\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0451 - val_loss: 0.0686\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0441 - val_loss: 0.0694\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0473 - val_loss: 0.0698\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0459 - val_loss: 0.0691\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0423 - val_loss: 0.0679\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0440 - val_loss: 0.0668\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0445 - val_loss: 0.0665\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0432 - val_loss: 0.0663\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6bc23a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:25.218864Z",
     "iopub.status.busy": "2023-06-24T16:57:25.218265Z",
     "iopub.status.idle": "2023-06-24T16:57:28.068130Z",
     "shell.execute_reply": "2023-06-24T16:57:28.067090Z",
     "shell.execute_reply.started": "2023-06-24T16:57:25.218834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04361634701490402"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c372d1a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:28.071249Z",
     "iopub.status.busy": "2023-06-24T16:57:28.070925Z",
     "iopub.status.idle": "2023-06-24T16:57:28.154302Z",
     "shell.execute_reply": "2023-06-24T16:57:28.153684Z",
     "shell.execute_reply.started": "2023-06-24T16:57:28.071214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06503254920244217"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "919f484c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:28.155753Z",
     "iopub.status.busy": "2023-06-24T16:57:28.155526Z",
     "iopub.status.idle": "2023-06-24T16:57:28.162877Z",
     "shell.execute_reply": "2023-06-24T16:57:28.161420Z",
     "shell.execute_reply.started": "2023-06-24T16:57:28.155733Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics(model,Y_train,Y_pred_train,Y_test,Y_pred_test):\n",
    "    print('Train MAE  value   : %.3f ' % mean_absolute_error(Y_train, Y_pred_train))\n",
    "    print('Train MSE  value   : %.3f ' % mean_squared_error(Y_train, Y_pred_train))\n",
    "    print('Train RMSE value   : %.3f ' % root_mean_squared_error(Y_train, Y_pred_train))\n",
    "    print('Train R2   value   : %.3f ' % r2_score(Y_train, Y_pred_train))\n",
    "    print('---------------------------------------------')\n",
    "    print('Test  MAE  value   : %.3f ' % mean_absolute_error(Y_test, Y_pred_test))\n",
    "    print('Test  MSE  value   : %.3f ' % mean_squared_error(Y_test, Y_pred_test))\n",
    "    print('Test  RMSE value   : %.3f ' % root_mean_squared_error(Y_test, Y_pred_test))\n",
    "    print('Test  R2   value   : %.3f ' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63c7d720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:28.164739Z",
     "iopub.status.busy": "2023-06-24T16:57:28.164245Z",
     "iopub.status.idle": "2023-06-24T16:57:28.180224Z",
     "shell.execute_reply": "2023-06-24T16:57:28.179110Z",
     "shell.execute_reply.started": "2023-06-24T16:57:28.164714Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error\n",
    "def root_mean_squared_error(y_true, y_pred):    \n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a4dab66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:28.181944Z",
     "iopub.status.busy": "2023-06-24T16:57:28.181662Z",
     "iopub.status.idle": "2023-06-24T16:57:29.446572Z",
     "shell.execute_reply": "2023-06-24T16:57:29.444511Z",
     "shell.execute_reply.started": "2023-06-24T16:57:28.181922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "--------------------------------------------------\n",
      "Train MAE  value   : 0.154 \n",
      "Train MSE  value   : 0.044 \n",
      "Train RMSE value   : 0.209 \n",
      "Train R2   value   : -0.111 \n",
      "---------------------------------------------\n",
      "Test  MAE  value   : 0.213 \n",
      "Test  MSE  value   : 0.065 \n",
      "Test  RMSE value   : 0.255 \n",
      "Test  R2   value   : -0.889 \n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test  = model.predict(X_test) \n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print_metrics(model , Y_train , y_pred_train , Y_test , y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60a18546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T16:57:29.448652Z",
     "iopub.status.busy": "2023-06-24T16:57:29.448328Z",
     "iopub.status.idle": "2023-06-24T16:57:29.673594Z",
     "shell.execute_reply": "2023-06-24T16:57:29.672143Z",
     "shell.execute_reply.started": "2023-06-24T16:57:29.448629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkXklEQVR4nO3deXwTZeIG8CdHc7ZN6V3a0pZT7qNVLIigQBFEwWNBcBEUVARdEY9dZJVDl/pzEWFXgWVVEM8qILKKQJFDlBuLICB3aYGW0pbebdok7++PNCmhB6UkmTZ9vp/PfJJMJjNvMqF5eK+RCSEEiIiIiDyEXOoCEBERETkTww0RERF5FIYbIiIi8igMN0RERORRGG6IiIjIozDcEBERkUdhuCEiIiKPwnBDREREHoXhhoiIiDwKw42HWrFiBWQymX1RKpWIiIjA448/jgsXLti327ZtG2QyGbZt23bDx9i5cydmz56NvLw85xW8UlJSEjp37gytVguZTIaDBw/WuN3NlJ9cZ8CAAejSpUuDX5+bm4tHHnkEwcHBkMlkGDlypPMKV4PFixdjxYoVLtu/TCbD7NmzXbZ/V4qOjsaECRPq3KakpASzZ892+b9DZ/3NmTBhAqKjoxv0Wld/V27UvHnzsHbtWqmL0egw3Hi45cuXY9euXUhOTsaTTz6JL774Av369UNxcfFN73vnzp2YM2eO08PN5cuXMW7cOLRp0wYbNmzArl270L59e6cegxq3N954A9988w3effdd7Nq1C2+//bZLj+fqH6xdu3Zh0qRJLtu/1EpKSjBnzhy3hBtX/M25EQw3TYNS6gKQa3Xp0gVxcXEAgLvuugtmsxlvvPEG1q5di0cffVTi0tXsxIkTqKiowJ///Gf0799f6uKQBH7//Xe0adPGad9RIQTKysqg1Wpvel8VFRX22tD6uv3222/6uERUf6y5aWZsf2TPnTtX53br1q1DfHw8dDodfHx8MHjwYOzatcv+/OzZs/Hyyy8DAGJiYuzNX9f7n9v19jthwgTccccdAIDRo0dDJpNhwIABN/w+r3ccwFpD9NRTTyEyMhJqtRpBQUHo27cvNm/ebN8mJSUFw4cPR3BwMNRqNVq2bIl7770X58+fr/XY06ZNg16vR0FBQbXnRo8ejZCQEFRUVAAAtmzZggEDBiAgIABarRatWrXCQw89hJKSkuu+x6SkJMTHx0Ov18Pb2xtDhgxBSkqKwzYTJkyAt7c3jhw5goEDB0Kv1yMoKAjPPvtstWOUlZVhxowZiImJgUqlQnh4OKZOnVrj/5I///xzxMfHw9vbG97e3ujRowc+/PDDatvt27cP/fr1g06nQ+vWrfHWW2/BYrHU+p5SU1Mhk8mwefNmHDt2rNr3Kjc3F1OmTEF4eDhUKhVat26NmTNnwmg0OuxHJpPh2WefxdKlS9GxY0eo1Wp8/PHHNR4zOjoaR44cwfbt2+3HszVZ2Jo9P/nkE7z44osIDw+HWq3GqVOncPnyZUyZMgWdOnWCt7c3goODcffdd2PHjh3VjnFts5St2Xjr1q145plnEBgYiICAADz44IO4ePFirZ+Pzf79+/HII48gOjoaWq0W0dHRGDNmTLV/1zdynIqKCrzyyisIDQ2FTqfDHXfcgb179163LKmpqQgKCgIAzJkzx/4ZXt2UdfLkSYwdO9b+76hjx454//33HfZjsVjw5ptvokOHDtBqtfDz80O3bt2waNEiAA3/m7NixQp06NDBftyVK1fWuN2cOXPQu3dv+Pv7w9fXF7169cKHH36Iq68tXdd3paysDC+++CJ69OgBg8EAf39/xMfH49tvv612rK+//hq9e/eGwWCw/9t44oknHLYpKCjASy+95PDvcdq0aQ617jKZDMXFxfj444/t5WnI30uPJMgjLV++XAAQ+/btc1i/aNEiAUAsW7ZMCCHE1q1bBQCxdetW+zafffaZACASEhLE2rVrRVJSkoiNjRUqlUrs2LFDCCFEenq6eO655wQAsWbNGrFr1y6xa9cukZ+fX2uZ6rPfU6dOiffff18AEPPmzRO7du0SR44cqXWfDS2/EEIMGTJEBAUFiWXLlolt27aJtWvXitdff118+eWXQgghioqKREBAgIiLixNfffWV2L59u0hKShKTJ08WR48erbVMv/32mwAg/vvf/zqsv3LlilCr1WL69OlCCCHOnj0rNBqNGDx4sFi7dq3Ytm2b+Oyzz8S4cePElStXat2/EEL84x//EDKZTDzxxBPiu+++E2vWrBHx8fFCr9c7fF7jx48XKpVKtGrVSvzjH/8QmzZtErNnzxZKpVIMHz7cvp3FYhFDhgwRSqVSvPbaa2LTpk1i/vz5Qq/Xi549e4qysjL7tq+99poAIB588EHx9ddfi02bNokFCxaI1157zb5N//79RUBAgGjXrp1YunSpSE5OFlOmTBEAxMcff1zr+yorKxO7du0SPXv2FK1bt3b4XpWWlopu3boJvV4v5s+fLzZt2iRee+01oVQqxbBhwxz2A0CEh4eLbt26ic8//1xs2bJF/P777zUe89dffxWtW7cWPXv2tB/v119/FUJUfb/Cw8PFww8/LNatWye+++47kZOTI/744w/xzDPPiC+//FJs27ZNfPfdd2LixIlCLpc7fB9t5Zk1a5b9se3fZ+vWrcVzzz0nNm7cKD744APRokULcdddd9V+4it9/fXX4vXXXxfffPON2L59u/jyyy9F//79RVBQkLh8+XKDjjN+/Hghk8nEyy+/bD+n4eHhwtfXV4wfP77Oc7ZhwwYBQEycONH+GZ46dUoIIcSRI0eEwWAQXbt2FStXrhSbNm0SL774opDL5WL27Nn2/SQmJgqFQiFmzZolfvzxR7FhwwaxcOFC+zYN+Ztje/8jRowQ//vf/8Snn34q2rZtKyIjI0VUVJTDthMmTBAffvihSE5OFsnJyeKNN94QWq1WzJkzx75NXd+VvLw8MWHCBPHJJ5+ILVu2iA0bNoiXXnpJyOVyh+/8zp07hUwmE4888ohYv3692LJli1i+fLkYN26cfZvi4mLRo0cPERgYKBYsWCA2b94sFi1aJAwGg7j77ruFxWIRQgixa9cuodVqxbBhw+zlqevvZXPCcOOhbP+od+/eLSoqKkRhYaH47rvvRFBQkPDx8RGZmZlCiOrhwGw2i5YtW4quXbsKs9ls319hYaEIDg4Wffr0sa/75z//KQCIs2fPXrc8N7JfW5m+/vrr6+73Zsrv7e0tpk2bVuu+9+/fLwCItWvXXrcc1+rVq5fDsYQQYvHixQKAOHz4sBBCiFWrVgkA4uDBgze077S0NKFUKsVzzz3nsL6wsFCEhoaKUaNG2deNHz9eABCLFi1y2PYf//iHACB+/vlnIYSw/zi9/fbbDtslJSU5hOEzZ84IhUIhHn300TrL2L9/fwFA7Nmzx2F9p06dxJAhQ677Hvv37y86d+7ssG7p0qUCgPjqq68c1v/f//2fACA2bdpkXwdAGAwGkZube91jCSFE586dRf/+/autt32/7rzzzuvuw2QyiYqKCjFw4EDxwAMPODxXW7iZMmWKw3Zvv/22ACAyMjLqVe6rj11UVCT0er3Dua7vcY4dOyYAiBdeeMFhO9t/FOoKN0IIcfny5Wrv0WbIkCEiIiKiWgh59tlnhUajsZ+j4cOHix49etR5nIb8zenVq5c9DAghRGpqqvDy8qoWbq59bUVFhZg7d64ICAhweH1t35Vr2b4PEydOFD179rSvnz9/vgAg8vLyan1tYmKikMvl1f5zavubsX79evs6vV5/3fPTHLFZysPdfvvt8PLygo+PD4YPH47Q0FD88MMPCAkJqXH748eP4+LFixg3bhzk8qqvh7e3Nx566CHs3r27Xk0m7trvzRzntttuw4oVK/Dmm29i9+7d9qYim7Zt26JFixb461//iqVLl+Lo0aP1Lsfjjz+OnTt34vjx4/Z1y5cvx6233mofRdSjRw+oVCo89dRT+Pjjj3HmzJl67Xvjxo0wmUx47LHHYDKZ7ItGo0H//v1rrKa/tu/K2LFjAQBbt24FYG0eA1BtVMyf/vQn6PV6/PjjjwCA5ORkmM1mTJ069brlDA0NxW233eawrlu3btdtEq3Nli1boNfr8fDDDzust5XZVkabu+++Gy1atGjQsa710EMP1bh+6dKl6NWrFzQaDZRKJby8vPDjjz/i2LFj9drv/fff7/C4W7duAK7fbFxUVIS//vWvaNu2LZRKJZRKJby9vVFcXFzjsa93HNv34NrvyahRo26ob9G1ysrK8OOPP+KBBx6ATqdz+L4OGzYMZWVl2L17NwDrv8fffvsNU6ZMwcaNG2ts1r0Rtr8FY8eOhUwms6+PiopCnz59qm2/ZcsWDBo0CAaDAQqFAl5eXnj99deRk5ODrKyseh3z66+/Rt++feHt7W3/Pnz44YcO5+TWW28FYP1sv/rqK4fRqzbfffcdunTpgh49ejh8ZkOGDOHo0HpiuPFwK1euxL59+5CSkoKLFy/i0KFD6Nu3b63b5+TkAADCwsKqPdeyZUtYLBZcuXLlhsvhqv3ezHGSkpIwfvx4fPDBB4iPj4e/vz8ee+wxZGZmAgAMBgO2b9+OHj164NVXX0Xnzp3RsmVLzJo1q1oQutajjz4KtVptH1Vx9OhR7Nu3D48//rh9mzZt2mDz5s0IDg7G1KlT0aZNG7Rp08bex6A2ly5dAmD9I+nl5eWwJCUlITs722F7pVKJgIAAh3WhoaEOn1dOTg6USqW974SNTCZDaGiofbvLly8DACIiIuosI4BqxwQAtVqN0tLS6762Jjk5OQgNDXX4oQKA4OBgKJVKexltavoONFRN+1qwYAGeeeYZ9O7dG6tXr8bu3buxb98+3HPPPfV+j9d+Rmq1GgCu+/qxY8fivffew6RJk7Bx40bs3bsX+/btQ1BQUI2vvd5xbJ+d7XthU9N350bk5OTAZDLh3//+d7Xv6rBhwwDA/n2dMWMG5s+fj927d2Po0KEICAjAwIEDsX///gYfu6b3VNO6vXv3IiEhAQDw3//+F7/88gv27duHmTNnArj++QCANWvWYNSoUQgPD8enn36KXbt2Yd++fXjiiSdQVlZm3+7OO+/E2rVr7f9BiYiIQJcuXfDFF1/Yt7l06RIOHTpU7TPz8fGBEKLav3GqjqOlPFzHjh3to6Xqw/aHLCMjo9pzFy9ehFwub9D/hl2135s5TmBgIBYuXIiFCxciLS0N69atw9/+9jdkZWVhw4YNAICuXbviyy+/hBAChw4dwooVKzB37lxotVr87W9/q7UcLVq0wIgRI7By5Uq8+eabWL58OTQaDcaMGeOwXb9+/dCvXz+YzWbs378f//73vzFt2jSEhITgkUceqXHfgYGBAIBVq1YhKirqup+JyWRCTk6Ow4+ULcDZ1gUEBMBkMuHy5csOAUcIgczMTPv/Nm3PnT9/HpGRkdc9tjMFBARgz549EEI4BJysrCyYTCb752JzbQi6GTXt69NPP8WAAQOwZMkSh/WFhYVOO25N8vPz8d1332HWrFkO30Gj0Yjc3NwG7dP2PcjMzER4eLh9ve2701AtWrSAQqHAuHHjaq3ti4mJAWANUtOnT8f06dORl5eHzZs349VXX8WQIUOQnp4OnU53Q8e++j1d69p1X375Jby8vPDdd99Bo9HY19/IEOtPP/0UMTExSEpKcvi+XNvZHQBGjBiBESNGwGg0Yvfu3UhMTMTYsWMRHR2N+Ph4BAYGQqvV4qOPPqrxWNd+16k61tyQgw4dOiA8PByff/65wyiB4uJirF692j4CCaj//zJvdL/uKv/VWrVqhWeffRaDBw/Gr7/+Wu15mUyG7t27491334Wfn1+N21zr8ccfx8WLF7F+/Xp8+umneOCBB+Dn51fjtgqFAr1797aPIKlr/0OGDIFSqcTp06cRFxdX43Ktzz77zOHx559/DgD2kRUDBw4EYP0DfbXVq1ejuLjY/nxCQgIUCkW1H3R3GDhwIIqKiqr94NhGv9jK2BANqVGSyWT2fwM2hw4dqjYqz9lkMhmEENWO/cEHH8BsNjdon7bvwbXfk6+++gomk+m6r6/tb4FOp8Ndd92FlJQUdOvWrcbvak01Q35+fnj44YcxdepU5ObmIjU1tc7j1KRDhw4ICwvDF1984fC34Ny5c9i5c6fDtrah/QqFwr6utLQUn3zySY3vtabjy2QyqFQqh2CTmZlZ42ipq/fVv39//N///R8A2Ec7Dh8+HKdPn0ZAQECNn9nVExDeTG2oJ2PNDTmQy+V4++238eijj2L48OF4+umnYTQa8c9//hN5eXl466237Nt27doVALBo0SKMHz8eXl5e6NChA3x8fG5qv+4of35+Pu666y6MHTsWt9xyC3x8fLBv3z5s2LABDz74IABru/fixYsxcuRItG7dGkIIrFmzBnl5eRg8ePB1y5KQkICIiAhMmTIFmZmZDk1SgLW/xpYtW3DvvfeiVatWKCsrs/9PbdCgQbXuNzo6GnPnzsXMmTNx5swZ3HPPPWjRogUuXbqEvXv3Qq/XY86cOfbtVSoV3nnnHRQVFeHWW2/Fzp078eabb2Lo0KH2YfeDBw/GkCFD8Ne//hUFBQXo27cvDh06hFmzZqFnz54YN26c/divvvoq3njjDZSWlmLMmDEwGAw4evQosrOzHY7rbI899hjef/99jB8/HqmpqejatSt+/vlnzJs3D8OGDavzM7seWw1dUlISWrduDY1GY/9+12b48OF44403MGvWLPTv3x/Hjx/H3LlzERMTU69A0FC+vr6488478c9//hOBgYGIjo7G9u3b8eGHH9Yanq+nY8eO+POf/4yFCxfCy8sLgwYNwu+//4758+fD19f3uq/38fFBVFQUvv32WwwcOBD+/v72si1atAh33HEH+vXrh2eeeQbR0dEoLCzEqVOn8L///c/e3+u+++6zz8sVFBSEc+fOYeHChYiKikK7du0A3PjfnDfeeAOTJk3CAw88gCeffBJ5eXmYPXt2tWape++9FwsWLMDYsWPx1FNPIScnB/Pnz68WIG1lqOm7Mnz4cKxZswZTpkzBww8/jPT0dLzxxhsICwvDyZMn7a9//fXXcf78eQwcOBARERHIy8vDokWL4OXlZZ/Xa9q0aVi9ejXuvPNOvPDCC+jWrRssFgvS0tKwadMmvPjii+jdu7e9PNu2bcP//vc/hIWFwcfHBx06dKjPafdsUvVkJteqbSj4tWoaSi2EEGvXrhW9e/cWGo1G6PV6MXDgQPHLL79Ue/2MGTNEy5YthVwur3E/16rPfm9mtFR9j1NWViYmT54sunXrJnx9fYVWqxUdOnQQs2bNEsXFxUIIIf744w8xZswY0aZNG6HVaoXBYBC33XabWLFixXXLZfPqq68KACIyMtJh9JYQ1mGcDzzwgIiKihJqtVoEBASI/v37i3Xr1tVr32vXrhV33XWX8PX1FWq1WkRFRYmHH35YbN682b7N+PHjhV6vF4cOHRIDBgwQWq1W+Pv7i2eeeUYUFRU57K+0tFT89a9/FVFRUcLLy0uEhYWJZ555psZh6StXrhS33nqr0Gg0wtvbW/Ts2VMsX77c/nxNo51s5alrlMr1Xp+TkyMmT54swsLChFKpFFFRUWLGjBkOQ9WFsI5Omjp16nWPY5OamioSEhKEj4+PAGAvY13fRaPRKF566SURHh4uNBqN6NWrl1i7dm2N7xG1jJa69t9nbd/na50/f1489NBDokWLFsLHx0fcc8894vfffxdRUVEOI2du5DhGo1G8+OKLIjg4WGg0GnH77beLXbt2VdtnbTZv3ix69uwp1Gp1tRFWZ8+eFU888YQIDw8XXl5eIigoSPTp00e8+eab9m3eeecd0adPHxEYGGifvmDixIkiNTXV4Tg3+jfngw8+EO3atRMqlUq0b99efPTRRzWeo48++kh06NBBqNVq0bp1a5GYmCg+/PDDaqOzavuuCCHEW2+9JaKjo4VarRYdO3YU//3vf8WsWbPE1T+13333nRg6dKgIDw8XKpVKBAcHi2HDhjlMUyGEdSqKv//976JDhw5CpVLZh9O/8MIL9tGuQghx8OBB0bdvX6HT6QSAeo3kag5kQlxVX0dEHmXChAlYtWoVioqKpC4KEZHbsM8NEREReRSGGyIiIvIobJYiIiIij8KaGyIiIvIoDDdERETkURhuiIiIyKM0u0n8LBYLLl68CB8fH6dOz05ERESuI4RAYWEhWrZs6XBh5Jo0u3Bz8eJFt18Th4iIiJwjPT39uhfvbXbhxjZNd3p6er2mFSciIiLpFRQUIDIyssbLbVyr2YUbW1OUr68vww0REVETU58uJexQTERERB6F4YaIiIg8CsMNEREReZRm1+eGiIjIlcxmMyoqKqQuRpOkUqmuO8y7PhhuiIiInEAIgczMTOTl5UldlCZLLpcjJiYGKpXqpvbDcENEROQEtmATHBwMnU7HiWJvkG2S3YyMDLRq1eqmPj+GGyIioptkNpvtwSYgIEDq4jRZQUFBuHjxIkwmE7y8vBq8H3YoJiIiukm2PjY6nU7ikjRttuYos9l8U/thuCEiInISNkXdHGd9fgw3RERE5FEYboiIiMgpoqOjsXDhQqmLwQ7FREREzdmAAQPQo0cPp4SSffv2Qa/X33yhbhLDjZOYLQKXC40wmsyICpD+xBIRETmDEAJmsxlK5fUjQ1BQkBtKdH1slnKSSwVluD3xRwx+9yepi0JERFQvEyZMwPbt27Fo0SLIZDLIZDKsWLECMpkMGzduRFxcHNRqNXbs2IHTp09jxIgRCAkJgbe3N2699VZs3rzZYX/XNkvJZDJ88MEHeOCBB6DT6dCuXTusW7fO5e+L4cZJdCoFAKDcZIHJbJG4NEREJDUhBErKTW5fhBD1LuOiRYsQHx+PJ598EhkZGcjIyEBkZCQA4JVXXkFiYiKOHTuGbt26oaioCMOGDcPmzZuRkpKCIUOG4L777kNaWlqdx5gzZw5GjRqFQ4cOYdiwYXj00UeRm5t7U5/t9bBZykm0leEGAEoqzPBVMDcSETVnpRVmdHp9o9uPe3TuEOhU9ft5NxgMUKlU0Ol0CA0NBQD88ccfAIC5c+di8ODB9m0DAgLQvXt3++M333wT33zzDdatW4dnn3221mNMmDABY8aMAQDMmzcP//73v7F3717cc889N/ze6ou/wE6iUsihkFvH55eW39zkQ0RERFKLi4tzeFxcXIxXXnkFnTp1gp+fH7y9vfHHH39ct+amW7du9vt6vR4+Pj7IyspySZltWHPjJDKZDDqVAoVlJpQw3BARNXtaLwWOzh0iyXGd4dpRTy+//DI2btyI+fPno23bttBqtXj44YdRXl5e536uvYyCTCaDxeLa7hsMN05kCzfFRpPURSEiIolZ/9Pb+H9mVSpVvS53sGPHDkyYMAEPPPAAAKCoqAipqakuLl3DsFnKiWxf4tIK1twQEVHTEB0djT179iA1NRXZ2dm11qq0bdsWa9aswcGDB/Hbb79h7NixLq+BaSiGGyeyjZhisxQRETUVL730EhQKBTp16oSgoKBa+9C8++67aNGiBfr06YP77rsPQ4YMQa9evdxc2vpp/PVlTYg93LBZioiImoj27dtj165dDusmTJhQbbvo6Ghs2bLFYd3UqVMdHl/bTFXTsPS8vLwGlfNGsObGibSVzVKsuSEiIpIOw40T6Sp7qJewzw0REZFkGG6cSKe2hpvScjZLERERSYXhxolsfW6Kjay5ISIikgrDjRNxKDgREZH0GG6cqGooOJuliIiIpMJw40RVQ8FZc0NERCQVhhsn4lBwIiIi6THcOBGHghMREUmP4caJ9BwKTkREJDmGGyeyNUtxKDgRETUVAwYMwLRp05y2vwkTJmDkyJFO219DMNw4ka1DMYeCExERSUfycLN48WLExMRAo9EgNjYWO3bsqHP7zz77DN27d4dOp0NYWBgef/xx5OTkuKm0deNQcCIiakomTJiA7du3Y9GiRZDJZJDJZEhNTcXRo0cxbNgweHt7IyQkBOPGjUN2drb9datWrULXrl2h1WoREBCAQYMGobi4GLNnz8bHH3+Mb7/91r6/bdu2uf19SRpukpKSMG3aNMycORMpKSno168fhg4dWuvl1n/++Wc89thjmDhxIo4cOYKvv/4a+/btw6RJk9xc8prZJvHjUHAiIoIQQHmx+5carsRdm0WLFiE+Ph5PPvkkMjIykJGRAS8vL/Tv3x89evTA/v37sWHDBly6dAmjRo0CAGRkZGDMmDF44okncOzYMWzbtg0PPvgghBB46aWXMGrUKNxzzz32/fXp08dVn3CtlG4/4lUWLFiAiRMn2sPJwoULsXHjRixZsgSJiYnVtt+9ezeio6Pxl7/8BQAQExODp59+Gm+//bZby10be81NhRlCCMhkMolLREREkqkoAea1dP9xX70IqPT12tRgMEClUkGn0yE0NBQA8Prrr6NXr16YN2+efbuPPvoIkZGROHHiBIqKimAymfDggw8iKioKANC1a1f7tlqtFkaj0b4/KUhWc1NeXo4DBw4gISHBYX1CQgJ27txZ42v69OmD8+fPY/369RBC4NKlS1i1ahXuvffeWo9jNBpRUFDgsLiKtjLcmC0C5WaLy45DRETkKgcOHMDWrVvh7e1tX2655RYAwOnTp9G9e3cMHDgQXbt2xZ/+9Cf897//xZUrVyQutSPJam6ys7NhNpsREhLisD4kJASZmZk1vqZPnz747LPPMHr0aJSVlcFkMuH+++/Hv//971qPk5iYiDlz5ji17LWxzXMDAKXlZqiVijq2JiIij+als9aiSHHcm2CxWHDffffh//7v/6o9FxYWBoVCgeTkZOzcuRObNm3Cv//9b8ycORN79uxBTEzMTR3bWSTvUHxt001dzTlHjx7FX/7yF7z++us4cOAANmzYgLNnz2Ly5Mm17n/GjBnIz8+3L+np6U4t/9WUCjlUSutHWsxZiomImjeZzNo85O7lBrtEqFQqmM1Vv1m9evXCkSNHEB0djbZt2zoser2+8q3J0LdvX8yZMwcpKSlQqVT45ptvatyfFCQLN4GBgVAoFNVqabKysqrV5tgkJiaib9++ePnll9GtWzcMGTIEixcvxkcffYSMjIwaX6NWq+Hr6+uwuJJ9ODhHTBERURMQHR2NPXv2IDU1FdnZ2Zg6dSpyc3MxZswY7N27F2fOnMGmTZvwxBNPwGw2Y8+ePZg3bx7279+PtLQ0rFmzBpcvX0bHjh3t+zt06BCOHz+O7OxsVFRUuP09SRZuVCoVYmNjkZyc7LA+OTm51p7VJSUlkMsdi6xQWMOEuIHe4a6k5/WliIioCXnppZegUCjQqVMnBAUFoby8HL/88gvMZjOGDBmCLl264Pnnn4fBYIBcLoevry9++uknDBs2DO3bt8ff//53vPPOOxg6dCgA4Mknn0SHDh0QFxeHoKAg/PLLL25/T5KOlpo+fTrGjRuHuLg4xMfHY9myZUhLS7M3M82YMQMXLlzAypUrAQD33XcfnnzySSxZsgRDhgxBRkYGpk2bhttuuw0tW0rQI70Gtk7FnKWYiIiagvbt22PXrl3V1q9Zs6bG7Tt27IgNGzbUur+goCBs2rTJaeVrCEnDzejRo5GTk4O5c+ciIyMDXbp0wfr16+1DyzIyMhzmvJkwYQIKCwvx3nvv4cUXX4Sfnx/uvvvuGjs9SaVqlmI2SxEREUlBJhpLe46bFBQUwGAwID8/3yX9b0b/Zxf2nM3Fe2N7Yni3xlGbRERErlVWVoazZ8/aZ9ynhqnrc7yR32/JR0t5Gr2afW6IiIikxHDjZLY+NyVGNksRERFJgeHGyWwT+ZXwyuBERM1OM+vp4XTO+vwYbpzM1ixVymYpIqJmw8vLC4B1yhJquPLycgBV07w0lKSjpTwRh4ITETU/CoUCfn5+yMrKAgDodDpePPkGWSwWXL58GTqdDkrlzcUThhsnszVLcSg4EVHzYrsKti3g0I2Ty+Vo1arVTQdDhhsns3coZrMUEVGzIpPJEBYWhuDgYEkuOeAJVCpVtSsRNATDjZNxKDgRUfOmUChuus8I3Rx2KHYynb3mhs1SREREUmC4cTKtF5uliIiIpMRw42Q6FYeCExERSYnhxsl06sqh4GyWIiIikgTDjZPZrwrOmhsiIiJJMNw4mc6Lo6WIiIikxHDjZLZmqdIKMywWXmOEiIjI3RhunMzWLCUEUGZi7Q0REZG7Mdw4mUZZNXETm6aIiIjcj+HGyeRymX2uG3YqJiIicj+GGxfQczg4ERGRZBhuXIAXzyQiIpIOw40L2IaDs1mKiIjI/RhuXMA2HJw1N0RERO7HcOMCvDI4ERGRdBhuXEDLWYqJiIgkw3DjAjp2KCYiIpIMw40L2IaClxjZLEVERORuDDcuYG+WqmDNDRERkbsx3LiArVmKQ8GJiIjcj+HGBaqGgrNZioiIyN0YblxA52W7/AJrboiIiNxN8nCzePFixMTEQKPRIDY2Fjt27Kh12wkTJkAmk1VbOnfu7MYSX59OxRmKiYiIpCJpuElKSsK0adMwc+ZMpKSkoF+/fhg6dCjS0tJq3H7RokXIyMiwL+np6fD398ef/vQnN5e8blpO4kdERCQZScPNggULMHHiREyaNAkdO3bEwoULERkZiSVLltS4vcFgQGhoqH3Zv38/rly5gscff9zNJa+bnpdfICIikoxk4aa8vBwHDhxAQkKCw/qEhATs3LmzXvv48MMPMWjQIERFRdW6jdFoREFBgcPiapyhmIiISDqShZvs7GyYzWaEhIQ4rA8JCUFmZuZ1X5+RkYEffvgBkyZNqnO7xMREGAwG+xIZGXlT5a4PDgUnIiKSjuQdimUymcNjIUS1dTVZsWIF/Pz8MHLkyDq3mzFjBvLz8+1Lenr6zRS3XvQcCk5ERCQZpVQHDgwMhEKhqFZLk5WVVa0251pCCHz00UcYN24cVCpVnduq1Wqo1eqbLu+N0FaOluJQcCIiIveTrOZGpVIhNjYWycnJDuuTk5PRp0+fOl+7fft2nDp1ChMnTnRlERvMNs9NuckCs0VIXBoiIqLmRbKaGwCYPn06xo0bh7i4OMTHx2PZsmVIS0vD5MmTAViblC5cuICVK1c6vO7DDz9E79690aVLFymKfV22oeCAtWnKR+MlYWmIiIiaF0nDzejRo5GTk4O5c+ciIyMDXbp0wfr16+2jnzIyMqrNeZOfn4/Vq1dj0aJFUhS5XtRKORRyGcwWgZJyM8MNERGRG8mEEM2q3aSgoAAGgwH5+fnw9fV12XG6ztqIQqMJW18agJhAvcuOQ0RE1BzcyO+35KOlPBVnKSYiIpIGw42L6NW8vhQREZEUGG5cRMsrgxMREUmC4cZFqmYpZrMUERGROzHcuEhVnxvW3BAREbkTw42L6DlLMRERkSQYblyEzVJERETSYLhxETZLERERSYPhxkU4FJyIiEgaDDcuUjUUnM1SRERE7sRw4yI6NksRERFJguHGRao6FDPcEBERuRPDjYvoOBSciIhIEgw3LsKh4ERERNJguHERDgUnIiKSBsONi9iapdjnhoiIyL0YblzE1izFoeBERETuxXDjIhwKTkREJA2GGxe5ullKCCFxaYiIiJoPhhsX0amtNTcmi0C52SJxaYiIiJoPhhsX0VVefgFgp2IiIiJ3YrhxEaVCDpXC+vGy3w0REZH7MNy4EOe6ISIicj+GGxfS28MNh4MTERG5C8ONC7HmhoiIyP0YblyIsxQTERG5H8ONC3GWYiIiIvdjuHEhzlJMRETkfgw3LsRmKSIiIvdjuHEhdigmIiJyP8nDzeLFixETEwONRoPY2Fjs2LGjzu2NRiNmzpyJqKgoqNVqtGnTBh999JGbSntjOBSciIjI/ZRSHjwpKQnTpk3D4sWL0bdvX/znP//B0KFDcfToUbRq1arG14waNQqXLl3Chx9+iLZt2yIrKwsmU+MMD9rKZinW3BAREbmPpOFmwYIFmDhxIiZNmgQAWLhwITZu3IglS5YgMTGx2vYbNmzA9u3bcebMGfj7+wMAoqOj3VnkG8IOxURERO4nWbNUeXk5Dhw4gISEBIf1CQkJ2LlzZ42vWbduHeLi4vD2228jPDwc7du3x0svvYTS0tJaj2M0GlFQUOCwuIuOzVJERERuJ1nNTXZ2NsxmM0JCQhzWh4SEIDMzs8bXnDlzBj///DM0Gg2++eYbZGdnY8qUKcjNza21301iYiLmzJnj9PLXh47NUkRERG4neYdimUzm8FgIUW2djcVigUwmw2effYbbbrsNw4YNw4IFC7BixYpaa29mzJiB/Px8+5Kenu7091AbW80Nh4ITERG5j2Q1N4GBgVAoFNVqabKysqrV5tiEhYUhPDwcBoPBvq5jx44QQuD8+fNo165dtdeo1Wqo1WrnFr6etGyWIiIicjvJam5UKhViY2ORnJzssD45ORl9+vSp8TV9+/bFxYsXUVRUZF934sQJyOVyREREuLS8DaFnsxQREZHbSdosNX36dHzwwQf46KOPcOzYMbzwwgtIS0vD5MmTAViblB577DH79mPHjkVAQAAef/xxHD16FD/99BNefvllPPHEE9BqtVK9jVpxEj8iIiL3k3Qo+OjRo5GTk4O5c+ciIyMDXbp0wfr16xEVFQUAyMjIQFpamn17b29vJCcn47nnnkNcXBwCAgIwatQovPnmm1K9hTpxKDgREZH7yYQQQupCuFNBQQEMBgPy8/Ph6+vr0mOl5ZTgzn9uhU6lwNG597j0WERERJ7sRn6/JR8t5clszVKlFWY0swxJREQkGYYbF7I1SwkBlFVYJC4NERFR88Bw40JaL4X9PoeDExERuQfDjQvJ5TJ7wGGnYiIiIvdguHExjpgiIiJyL4YbF+MsxURERO7FcONitlmKeX0pIiIi92C4cTFbzU0xww0REZFbMNy4mI7NUkRERG7FcONitnDDZikiIiL3YLhxMV1lnxs2SxEREbkHw42LVdXcsFmKiIjIHRhuXEzLeW6IiIjciuHGxWxDwRluiIiI3IPhxsU4iR8REZF7Mdy4GC+/QERE5F4MNy7GoeBERETuxXDjYlVDwdksRURE5A4MNy7GmhsiIiL3YrhxMQ4FJyIici+GGxfjUHAiIiL3YrhxMV44k4iIyL0YblyMzVJERETuxXDjYrbRUkaTBWaLkLg0REREno/hxsVszVIAm6aIiIjcgeHGxdRKOeQy630OByciInI9hhsXk8lk9qYp9rshIiJyPYYbN2CnYiIiIvdhuHEDPYeDExERuQ3DjRto2SxFRETkNpKHm8WLFyMmJgYajQaxsbHYsWNHrdtu27YNMpms2vLHH3+4scQ3TsdmKSIiIreRNNwkJSVh2rRpmDlzJlJSUtCvXz8MHToUaWlpdb7u+PHjyMjIsC/t2rVzU4kbhrMUExERuY+k4WbBggWYOHEiJk2ahI4dO2LhwoWIjIzEkiVL6nxdcHAwQkND7YtCoahze6mx5oaIiMh9JAs35eXlOHDgABISEhzWJyQkYOfOnXW+tmfPnggLC8PAgQOxdetWVxbTKWxDwTnPDRERkesppTpwdnY2zGYzQkJCHNaHhIQgMzOzxteEhYVh2bJliI2NhdFoxCeffIKBAwdi27ZtuPPOO2t8jdFohNFotD8uKChw3puoJw4FJyIich/Jwo2NTCZzeCyEqLbOpkOHDujQoYP9cXx8PNLT0zF//vxaw01iYiLmzJnjvAI3AIeCExERuY9kzVKBgYFQKBTVammysrKq1ebU5fbbb8fJkydrfX7GjBnIz8+3L+np6Q0uc0NxKDgREZH7SBZuVCoVYmNjkZyc7LA+OTkZffr0qfd+UlJSEBYWVuvzarUavr6+Dou7sUMxERGR+0jaLDV9+nSMGzcOcXFxiI+Px7Jly5CWlobJkycDsNa6XLhwAStXrgQALFy4ENHR0ejcuTPKy8vx6aefYvXq1Vi9erWUb+O62CxFRETkPpKGm9GjRyMnJwdz585FRkYGunTpgvXr1yMqKgoAkJGR4TDnTXl5OV566SVcuHABWq0WnTt3xvfff49hw4ZJ9Rbqhc1SRERE7iMTQgipC+FOBQUFMBgMyM/Pd1sT1frDGZjy2a+4LdofX02Od8sxiYiIPMmN/H5LfvmF5sA+FLyCzVJERESuxnDjBnpbs5SRzVJERESuxnDjBhwtRURE5D4MN26g5WgpIiIit2lQuPn444/x/fff2x+/8sor8PPzQ58+fXDu3DmnFc5T6DlaioiIyG0aFG7mzZsHrVYLANi1axfee+89vP322wgMDMQLL7zg1AJ6AlvNjckiUG6ySFwaIiIiz9ageW7S09PRtm1bAMDatWvx8MMP46mnnkLfvn0xYMAAZ5bPI9j63ADWK4OrlGwNJCIicpUG/cp6e3sjJycHALBp0yYMGjQIAKDRaFBaWuq80nkIL4UcXgrrxUA5HJyIiMi1GlRzM3jwYEyaNAk9e/bEiRMncO+99wIAjhw5gujoaGeWz2PoVErkl1agmMPBiYiIXKpBNTfvv/8+4uPjcfnyZaxevRoBAQEAgAMHDmDMmDFOLaCnsDVNlbJTMRERkUs1qObGz88P7733XrX1c+bMuekCeSoOByciInKPBtXcbNiwAT///LP98fvvv48ePXpg7NixuHLlitMK50k4HJyIiMg9GhRuXn75ZRQUFAAADh8+jBdffBHDhg3DmTNnMH36dKcW0FNoOUsxERGRWzSoWers2bPo1KkTAGD16tUYPnw45s2bh19//RXDhg1zagE9hY7NUkRERG7RoJoblUqFkpISAMDmzZuRkJAAAPD397fX6JAje4fiCtbcEBERuVKDam7uuOMOTJ8+HX379sXevXuRlJQEADhx4gQiIiKcWkBPoavsc8Oh4ERERK7VoJqb9957D0qlEqtWrcKSJUsQHh4OAPjhhx9wzz33OLWAnqJqKDibpYiIiFypQTU3rVq1wnfffVdt/bvvvnvTBfJU7FBMRETkHg0KNwBgNpuxdu1aHDt2DDKZDB07dsSIESOgUCiu/+JmyDYUvJjhhoiIyKUaFG5OnTqFYcOG4cKFC+jQoQOEEDhx4gQiIyPx/fffo02bNs4uZ5PHZikiIiL3aFCfm7/85S9o06YN0tPT8euvvyIlJQVpaWmIiYnBX/7yF2eX0SOwWYqIiMg9GlRzs337duzevRv+/v72dQEBAXjrrbfQt29fpxXOk3AoOBERkXs0qOZGrVajsLCw2vqioiKoVKqbLpQnqhoKzmYpIiIiV2pQuBk+fDieeuop7NmzB0IICCGwe/duTJ48Gffff7+zy+gRdGyWIiIicosGhZt//etfaNOmDeLj46HRaKDRaNCnTx+0bdsWCxcudHIRPQObpYiIiNyjQX1u/Pz88O233+LUqVM4duwYhBDo1KkT2rZt6+zyeQzOUExEROQe9Q4317va97Zt2+z3FyxY0OACeSoOBSciInKPeoeblJSUem0nk8kaXBhPZh8KXmGGEIKfExERkYvUO9xs3brVleXweLZmKSEAo8kCjRdnciYiInKFBnUophunvSrMcDg4ERGR60gebhYvXoyYmBhoNBrExsZix44d9XrdL7/8AqVSiR49eri2gE6ikMug8bJ+3BwOTkRE5DqShpukpCRMmzYNM2fOREpKCvr164ehQ4ciLS2tztfl5+fjsccew8CBA91UUuewNU1xODgREZHrSBpuFixYgIkTJ2LSpEno2LEjFi5ciMjISCxZsqTO1z399NMYO3Ys4uPj3VRS57A1TbFZioiIyHUkCzfl5eU4cOAAEhISHNYnJCRg586dtb5u+fLlOH36NGbNmuXqIjqdXm0bDs6aGyIiIldp0CR+zpCdnQ2z2YyQkBCH9SEhIcjMzKzxNSdPnsTf/vY37NixA0pl/YpuNBphNBrtjwsKChpe6JukrWyWYp8bIiIi15G8Q/G1873UNgeM2WzG2LFjMWfOHLRv377e+09MTITBYLAvkZGRN13mhtJ5Vc11Q0RERK4hWbgJDAyEQqGoVkuTlZVVrTYHAAoLC7F//348++yzUCqVUCqVmDt3Ln777TcolUps2bKlxuPMmDED+fn59iU9Pd0l76c+bM1SJexzQ0RE5DKSNUupVCrExsYiOTkZDzzwgH19cnIyRowYUW17X19fHD582GHd4sWLsWXLFqxatQoxMTE1HketVkOtVju38A3EZikiIiLXkyzcANbrVY0bNw5xcXGIj4/HsmXLkJaWhsmTJwOw1rpcuHABK1euhFwuR5cuXRxeHxwcDI1GU219Y2VrluJQcCIiIteRNNyMHj0aOTk5mDt3LjIyMtClSxesX78eUVFRAICMjIzrznnTlNiuL8Wh4ERERK4jE0IIqQvhTgUFBTAYDMjPz4evr69bj/3PjX/g/a2nMaFPNGbf39mtxyYiImrKbuT3W/LRUs2JfYZi9rkhIiJyGYYbN9JyKDgREZHLMdy4EYeCExERuR7DjRtxKDgREZHrMdy4EWcoJiIicj2GGzfSqdgsRURE5GoMN26kU7NZioiIyNUYbtzIVnPDGYqJiIhch+HGjexDwcvZLEVEROQqDDdupK9sliqrsMBsaVYTQxMREbkNw40b2ZqlADZNERERuQrDjRuplXLIZNb7bJoiIiJyDYYbN5LJZFVz3RhZc0NEROQKDDduxuHgRERErsVw42ZVw8HZLEVEROQKDDduVjUcnDU3RERErsBw42a24eDF7HNDRETkEgw3bsZmKSIiItdiuHEzNksRERG5FsONm1VdGZzhhoiIyBUYbtyMQ8GJiIhci+HGzeyT+LHPDRERkUsw3LiZvUMxa26IiIhcguHGzXQcCk5ERORSDDduxqHgRERErsVw42YcCk5ERORaDDduplNVjpZisxQREZFLMNy4mU7N0VJERESuxHDjZjo2SxEREbkUw42b2ZqlOBSciIjINRhu3MzWLFVsZLMUERGRK0gebhYvXoyYmBhoNBrExsZix44dtW77888/o2/fvggICIBWq8Utt9yCd999142lvXlVQ8FZc0NEROQKSikPnpSUhGnTpmHx4sXo27cv/vOf/2Do0KE4evQoWrVqVW17vV6PZ599Ft26dYNer8fPP/+Mp59+Gnq9Hk899ZQE7+DG6bysH3mFWaDcZIFKKXm+JCIi8igyIYSQ6uC9e/dGr169sGTJEvu6jh07YuTIkUhMTKzXPh588EHo9Xp88skn9dq+oKAABoMB+fn58PX1bVC5b0a5yYL2f/8BAPDb6wkw6LzcXgYiIqKm5kZ+vyWrNigvL8eBAweQkJDgsD4hIQE7d+6s1z5SUlKwc+dO9O/fv9ZtjEYjCgoKHBYpqZRyeClkADgcnIiIyBUkCzfZ2dkwm80ICQlxWB8SEoLMzMw6XxsREQG1Wo24uDhMnToVkyZNqnXbxMREGAwG+xIZGemU8t8MzlJMRETkOpJ3+JDJZA6PhRDV1l1rx44d2L9/P5YuXYqFCxfiiy++qHXbGTNmID8/376kp6c7pdw3g8PBiYiIXEeyDsWBgYFQKBTVammysrKq1eZcKyYmBgDQtWtXXLp0CbNnz8aYMWNq3FatVkOtVjun0E7SQq9CZkEZDl/IR5dwg9TFISIi8iiS1dyoVCrExsYiOTnZYX1ycjL69OlT7/0IIWA0Gp1dPJd6qFc4AOA/20/DbJGsPzcREZFHknQo+PTp0zFu3DjExcUhPj4ey5YtQ1paGiZPngzA2qR04cIFrFy5EgDw/vvvo1WrVrjlllsAWOe9mT9/Pp577jnJ3kNDjLmtFd7begqpOSVYfzgD93VvKXWRiIiIPIak4Wb06NHIycnB3LlzkZGRgS5dumD9+vWIiooCAGRkZCAtLc2+vcViwYwZM3D27FkolUq0adMGb731Fp5++mmp3kKD6NVKPNE3BguST+D9racwvFvYdfsZERERUf1IOs+NFKSe58Ymv6QCfd76EcXlZnw0IQ5331J3PyMiIqLmrEnMc9PcGXRe+HO8tYbqvS2n0MwyJhERkcsw3Eho4h0xUCnl+DUtD7vP5EpdHCIiIo/AcCOhYB8NRsdZJxVcvO2UxKUhIiLyDAw3EnvqztZQyGXYcTIbv6XnSV0cIiKiJo/hRmKR/jqM6GEdCs7aGyIiopvHcNMITBnQBjIZsPHIJZy8VCh1cYiIiJo0hptGoG2wD+7pHAoAWLLttMSlISIiatoYbhqJKQPaAgC+/e0i0nNLJC4NERFR08Vw00h0jTDgzvZBMFsElm5n7Q0REVFDMdw0IlMHtAEAfL3/PLIKyiQuDRERUdPEcNOI3Bbjj7ioFig3W/DBz2elLg4REVGTxHDTiMhkMky9y9r35tPd55BXUi5xiYiIiJoehptGZkCHIHQK80VJuRkrdqZKXRwiIqImh+Gmkbm69mb5L6koMpokLhEREVHTwnDTCN3TJRStA/XIL63AF3vSpC4OERFRk8Jw0wgp5DJMrhw5tWzHGZRVmCUuERERUdPBcNNIjewRjpYGDS4XGvFNygWpi0NERNRkMNw0UiqlHBP6RgMAVh84L21hiIiImhCGm0bs/u7hkMmA/eeu8JIMRERE9cRw04iFGjSIbx0AAPj2IJumiIiI6oPhppEb2TMcALD24EUIISQuDRERUePHcNPI3dMlFCqlHKeyinDkYoHUxSEiImr0GG4aOV+NFwZ1DAYArOWoKSIioutiuGkCRvawNk2t++0izBY2TREREdWF4aYJGNAhGAatF7IKjdh9Jkfq4hARETVqDDdNgEopx7CuYQDACf2IiIiug+GmiXigctTUht8zeTkGIiKiOjDcNBFxUS0Q7qdFkdGEH49lSV0cIiKiRovhpomQy2UY0aMlADZNERER1YXhpgmxTei3/UQW8krKJS4NERFR4yR5uFm8eDFiYmKg0WgQGxuLHTt21LrtmjVrMHjwYAQFBcHX1xfx8fHYuHGjG0srrfYhPugY5osKs8D3hzOkLg4REVGjJGm4SUpKwrRp0zBz5kykpKSgX79+GDp0KNLS0mrc/qeffsLgwYOxfv16HDhwAHfddRfuu+8+pKSkuLnktUjfBxxe5dJDPNDT2jTFCf2IiIhqJhMSXrCod+/e6NWrF5YsWWJf17FjR4wcORKJiYn12kfnzp0xevRovP766/XavqCgAAaDAfn5+fD19W1QuWt0dgfw8XBAbQCm/QZoWzhv31fJzC9D/Fs/Qgjg57/ehYgWOpcch4iIqDG5kd9vyWpuysvLceDAASQkJDisT0hIwM6dO+u1D4vFgsLCQvj7+9e6jdFoREFBgcPiElF9geDOgDEf+OVfrjkGrFcKvz3GdqXwiy47DhERUVMlWbjJzs6G2WxGSEiIw/qQkBBkZmbWax/vvPMOiouLMWrUqFq3SUxMhMFgsC+RkZE3Ve5ayeXAwNes9/csBQovueY4qJrzZm3KBV4pnIiI6BqSdyiWyWQOj4UQ1dbV5IsvvsDs2bORlJSE4ODgWrebMWMG8vPz7Ut6evpNl7lW7e8BIm4FKkqAHe+47DD3dLVeKfxkVhGOZvBK4URERFeTLNwEBgZCoVBUq6XJysqqVptzraSkJEycOBFfffUVBg0aVOe2arUavr6+DovLyGTAwMq+P/s/AvJq7hh9s3w1Xhh4izXQsWmKiIjIkWThRqVSITY2FsnJyQ7rk5OT0adPn1pf98UXX2DChAn4/PPPce+997q6mDcu5k6g9QDAUgFs+z+XHcY25823By/wSuFERERXkbRZavr06fjggw/w0Ucf4dixY3jhhReQlpaGyZMnA7A2KT322GP27b/44gs89thjeOedd3D77bcjMzMTmZmZyM/Pl+ot1Ozuytqb3z4HLh93ySEGdAiCr0aJSwVG7OGVwomIiOwkDTejR4/GwoULMXfuXPTo0QM//fQT1q9fj6ioKABARkaGw5w3//nPf2AymTB16lSEhYXZl+eff16qt1CziFjgluGAsABb/+GSQ6iVCtzbzXql8LUHOecNERGRjaTz3EjBZfPcXOvSUWBJHwACeGob0LKn0w+x50wORi/bDR+1Evv+PggaL4XTj0FERNQYNIl5bjxeSCegW+UQ9S1vNnw/xqJan7o12h8tDRoUGk3Y8gevFE5ERAQw3LjWgL8BciVwajOQ+suNvdZcAXw7FUgMB1ZNBIqqhxe5XIb7e1TNeUNEREQMN67l3xroVdkhessbQH1bAMtLgKQ/AymfWh//vgp4Lw44sAKwWBw2tU3ot/U4rxROREQEMNy43p0vA0oNkLbLWoNzPaVXgE8fBE5ssL7unreAsO5AWT7wv+eBFcOArD/sm3cI9cEtoT6oMAusP1y/mZ2JiIg8GcONq/m2BG570nr/x7nVal4cFGQAy++1BiG1ARi3Frj9GWDSFmDIPMBLb31u6R3WfjwVpQCq5rz5z0+nUVhW4eI3RERE1Lgx3LhD3xcAlQ+QeQg49m3N2+ScBj5KALKOAN4hwOPrgah463MKJRA/FZi6B2g/1DpB4E//tI7GOr0VY25thZYGDc7llODVb37n9aaIiKhZY7hxB30A0OdZ6/0t/wDMJsfnLx4EPhpivVyDf2tg4iYgtEv1/fhFAmO+AEZ9AviEAblngE9GwrBhKpY82AoKuQz/++0ivtjrwutnERERNXIMN+5y+xRA6w/knAQOfVm1/uxPwIrhQPFlILQb8MRGoEV07fuRyYBO9wNT9wK3PQ1ABhxKQve1g/F+L+t1pub87wiO8YKaRETUTDHcuIvGF+g33Xp/21uAyQgcXQd8+hBQXghE9wMmfAd4136F82r7G/Y2MOlHILQrUHoFQ47+FS9EnoTRZMHUz39FsdF0/f0QERF5GIYbd7p1krU5KT8dSBoHfD0eMJdbL9Xw6CpAY7jxfUbEAk9uA7o9ApnFhL/k/gMjvI/hzOVi/H0t+98QEVHzw3DjTl5aoP8r1vsnN1qvPdXrMWDUSsBL0/D9KpTAiPeBTiMgM5fjXfE2+iiO4ZuUC/h6/3nnlJ2IiKiJYLhxt57jgIB21vv9XgTu+xcgd8I1oRRK4MEPgPb3QG424mP1fPSSncDr637H8czCm98/ERFRE8ELZ0qhJBfIPw+EdXP+vivKgC9GA2e2oUSmw6iyV1EW1A3rnu0LnUrp/OMRERG5AS+c2djp/F0TbABr89YjnwOt+kAnSvCZ+i3ILx/Da2uPuOZ4REREjQzDjSdS6YGxSUB4LAwowmeqeUhJ2YtVB9j/hoiIPB/DjafS+AJ/Xg2EdkWQLB+fqeZh6dotOHmJ/W+IiMizMdx4Mm0LYNxaiKBbECbLxXLZXMz+dCNKy81Sl4yIiMhlGG48nT4Qsse+hckvBpHyy3gjfybmfbUVFeY6LuBJRETUhDHcNAc+oVBO+B/K9OFoLc/EhBPP4vn3V+NCXqnUJSMiInI6hpvmwi8SmonfoVQbijbyDPwjZxpmL1qMH49dkrpkRERETsVw05z4t4Z2ynYYQ3qihawISyxvYuuniUhcf4zNVERE5DEYbpobn1CoJ22AucsoKGUWvOm1HBE7/46xS3fgIpupiIjIAzDcNEdeGigeWgYMmg0BGcYpN+OFzL9hzKL12PIHm6mIiKhpY7hprmQy4I4XIBvzBSxeevRRHMVK89+Q+PFaJP7AZioiImq6GG6auw5DIZ+0GcIvClHyLKxRzcLJHavwyLLdbKYiIqImieGGgJBOkD25FYi6Az6yUnzg9Q7izq/EkHe34/Vvf8fvF/KlLiEREVG98argVMVUDvzwCnBgOQBgjfkOvF4xAUXQoXNLX4yKi8TIHuEw6LwkLigRETU3N/L7zXBDjoQA9n0A8cNfIRNm5CqD8UrZ49hs6g4AUCnlGNI5FKPjItGnTQDkcpnEBSYiouaA4aYODDf1lPoz8O1U4EoqAOBU2L14tXgM9mZVtWSG+2nxp7gI/CkuEuF+WokKSkREzQHDTR0Ybm5AeTGwdR6wezEgLBC6QKT3no1lud3x7W8ZKCwzAbAOvHqoVwReSuiAUING4kITEZEnupHfb8k7FC9evBgxMTHQaDSIjY3Fjh07at02IyMDY8eORYcOHSCXyzFt2jT3FbQ5UumBIf8AJiYDQR0hK8lGq63P4s2yROx7rhMWju6B+NYBEAJYdeA87pq/DQuST6DYaJK65ERE1IxJGm6SkpIwbdo0zJw5EykpKejXrx+GDh2KtLS0Grc3Go0ICgrCzJkz0b17dzeXthmLiAOe/gkYMAOQewHH10OzrA9GWpLxxZO9sWZKH/Rq5YfSCjP+9eNJ3DV/G5L2pcFsaVaVgkRE1EhI2izVu3dv9OrVC0uWLLGv69ixI0aOHInExMQ6XztgwAD06NEDCxcuvKFjslnqJl06Cqx7FrhwwPo4uh9w/78gWsRg/eFMvLXhGNJzrfPj3BLqg5n3dkS/dkESFpiIiDxBk2iWKi8vx4EDB5CQkOCwPiEhATt37nTacYxGIwoKChwWugkhnazNVEPmAUotkLoDWNwHsh/+invDS7B5en/8/d6O8NUo8UdmIcZ9uBcTlu/FiUuFUpeciIiaCcnCTXZ2NsxmM0JCQhzWh4SEIDMz02nHSUxMhMFgsC+RkZFO23ezJVcA8VOBKbuAmDsBUymw9z/Av2Oh/upRTApPw/aXBuDxvtFQymXYdvwy7ln4E1795jCOZRSwuYqIiFxKKXUBZDLHeVKEENXW3YwZM2Zg+vTp9scFBQUMOM7iHwM8tg44sxXYvRQ4uRE48QNw4ge0CO6EWbc/g/F/GYbE5FRsPHIJn+9Jw+d70uCjUSI2qgXiologLtof3SP8oFUppH43NSsvtg6Hv5IK5J6tun8lFdD5A63vAtrcDYTHAgrJ/zkREREkDDeBgYFQKBTVammysrKq1ebcDLVaDbVa7bT90TVkMuuPe5u7gexTwJ6lwMHPgayjwLrnEK2bjf/EPo6UXg9j4Z5C7E/NRWGZCduOX8a245cBAEq5DF3CDfawExfdAoHebj5n5SXA+b1A2h4g93RVmCnOqv01OQDS9wDb3wLUvtZarDZ3A20HAi2i3VRwIiK6luQdimNjY7F48WL7uk6dOmHEiBHsUNyUleYBKZ8Ae5YB+ZUj3+RKoPODMEfdgXSTAQfzNNiVpcL2dBMyCyuq7aJnKz+8MKg9+rULdGpNnp2x0Bpkzv0MpP4CXEwBLNXLAQDQ+FlrqVpEAy0qb/1aAfnpwOktwOmtQFme42v8W1eFvuh+gIbfNSKim9FkJvFLSkrCuHHjsHTpUsTHx2PZsmX473//iyNHjiAqKgozZszAhQsXsHLlSvtrDh48CACYNGkSOnTogJdffhkqlQqdOnWq1zEZbtzIbAKOfw/sXgKk7apxEyH3glkfjAJlIC5a/HC61AfHi/UogA4mKBAZ6It7ukWiTYgfoFABCi9rUFJ4WYelK1TW5iC5l3Wdfb1tu8rXlBdba1lSfwbO/QJk/AYIi2NhfMOBqD5ASOergkwUoG1R9/u0mIGMg8CpLdawc34vYLlqrh+ZAmjZA4i+wxp0Ins3rrBjMQMluYC53LpYTFX3zZX3LRWAuTL86YMA35aALhCQSz5VFhE1E00m3ADWSfzefvttZGRkoEuXLnj33Xdx5513AgAmTJiA1NRUbNu2zb59Tf+Lj4qKQmpqar2Ox3AjkYsp1uaqK6lAYQZQmAkUX5a2TH5R1sAR1QeI6msNNM6oJSorsIao01uA0z8CuWccn5cy7JTlA5eOAJm/A5cOA5mHgaxjgKnsxvclVwLeoYBPKOAbBviEWe/7tLTeBrazBkZX1LwRUbPTpMKNuzHcNCLmCqDokjXo2AJPYQZQkAGUF6HMaMS5y/nIzi+CEiYoYUaAVo5QbwU0cktVzYKtVsF2a7t/rYC21hAT1ReI7gsYItzzPvPPW5u+UndYQ8+Vs47P28JOq3hrmfRBgHcwoA+23te2qH8NicUClBdam93KCqz9hzIPV4WZvJonyATgWBOmUDnWhilU1jADARRlWRfU40+H2hcI7li5dKq61QfW7/0QEVViuKkDw03Tk5ZTgoU/nsDalAuwCGtFwP3dW2LaoPaICdTX/CIhKptXbEFH1niagq4Xdq4lV1qbgLyDKgNPoDXY2QKMsRAw2m4Lcd3Q4RsBhHYBQroAoV2tS4uYG2tiMpusna0LMiqDaUZVQC24CBRcsNZYWWq5FIc+qCroBHUAAtpZa3q8Q1jTQ0Q1YripA8NN03UqqxALkk9g/WHrCDuFXIYR3VsioXMo+rYNgI/GS+ISNpAt7Fz81VqTVXTZGhyKsqp3VK4vudJaa2KIsIYXW5AJ6Wwdwu4OpnIg55R15FzWscrbo/YrzddI5QMEtq0KOwFtrbf+bQCVzj3ldqaKMqAos7JWsnIpygSKsyuDd2XNo8Vc82NhBrT+1mY/33Br059vy6pbbQuGQWo2GG7qwHDT9P1+IR8Lkk9gyx9Vw7SVchl6tWqBO9sH4s72QejS0gC53AP+6JvKrX2TbEtRFlCSbW0mUvtaa6PUPtb7Vz9Wahrvj56xCMg+bg08l44C2SeAnJPWJrNrO3lfzTccMEQCfpHW0GaIdHys9nFP+StKreGk+DJQklN5brKrzlFhBlB4yXrb0HBaX0pNVT8nv0hrDZx/68rRfTHWWr7G+j0gukEMN3VguPEcv6ZdwbqDF/HTics4k13s8FyAXoU72gWif/sg9GsXhCAf67w5RpMZWQVGZOSXISO/FBn5ZcisvJ+ZX4asQiO81UqEGjQI8dUg1FeDEIP11npfjUC92jOCU2NjMlqbsrJPWsNO9qnK25P1CwkaP2vY8W1pvaK9lw7w0lYuV9+vfKzUAGajNayUlwAVxZX3K28rSirvl1inN7CFmfKiG3tfSo21uc0nDPCpvNUHAgr1VSP/FNY+Tg6PlYBMbg1OhRnW5j5bn7TCi9ayXI/KB/CPrgw9MVUjAL1DrWXS+TP8UJPBcFMHhhvPlJ5bgu0nLuOnE5ex83QOioyOfT1iAvUoLKtAdlH5TR9LKZch2EeNrhEGPNQrAnfdEgwvBYdEu4wQ1h/y3LPWuYXyz1fd5qVb77u6huRaCpW1H5TetgRVPbaPGqsMMxo/1wQIk9Ex7Fw5Z+2/lVu5FFzAdftfyb2sIcc72Fpm75DKIBZifU9KLaBUWQOaQgUo1dZFoXa8r/BqviHJYraGYVOZdakos16SpqLysclobW4U5spmR7O1htJ23/acsFSGW/VVn3nlfYdbtTWcq3SAl75ZzYzOcFMHhhvPV2G24NdzV/DTycv46UQ2Dl/Id3herZQjzKBBqEGDMIO28tZaMxPko0aR0YTM/DJcKihDZkEZMvON9vvZRUZc+y8mQK/CyJ7heDg2Ah3D+J2SRFmB9cc8L936g2+reakorfyhKXVcZ1tsPxRe2sranspanavve+kArZ9jmFH7Nv4f84oya1Nf7pmrQs8ZaygsugSU5jr3eApV1bxSCvVV91WVP8yqys/Zp7Ip1dt6q/KubFa9+rGPdVul5qpaN93NhyghrqqVK7LWzFVbCqs65xsLrc2oV3fYty3lxdbwUtvkn+6i1Fi/ryq99bO7+r7tu3z14qW7arvK+15a66hNueKqW/k1j223MutzqLx1WGSO952M4aYODDfNT3aREX9kFMJfr0KYQQM/nVeDZz02mS24XGTEhSul2HT0Etb8egHZRUb7813CffFwrwiM6BGOFnqVs94CkfOZjFXD+m2dnu33L1n7dpmM1sVstPb/MpVZR+qZjNL8qMsUjk2MSrU1sEBYaz7sC655bKlscixCvaYwaCiFylrb5aWprNmqLKNceVVTo8I6MlGmqFpvW2c2VX3WZttnX/m529eVW8OZMLvufTiDdwjw0gmn7pLhpg4MN+RMFWYLfjpxGasOnMfmY5dQYbb+c/JSyDCoYwj+FBeBO9sFQclmK/I0FstVP8AVlfNOXX2/vPIHubxyndHat6n86hqRwsrHtvtFVc+Zrqptq6ujeUPZa+hqqMlQ2zrqX71cVbtkq2FSaqpCllJjDSruIIT1c69WA1XT/au2sW9v609WXNXHzGKubDqzNZtd/dh84+fAJwx48Q+nvm2Gmzow3JCr5BaXY93BC/j6wHkcuVhgX69WyuGj8YK3WgG9WmldVNb73rbHaiW81Qr4aVUw6Lxg0HrBT+cFP60KfjovaLwa6VXTiVxNCGs4sjcpXtW0aCqroTlEXnPTiZe2KsR46XjpkBslbLVj19SSVas1E7C33esDnFoEhps6MNyQOxy9WIBVB85j7cELyC2++U7MaqXcHnYMWi94KWWQy2SQyWRQyGC/L6+8L5dbb8P9tLi9dQDiolvc1DxAV4rLsf/cFZy5XIRQgwbRAXpEBejgp2PTGxG5B8NNHRhuyJ0qzBZczCtFkdGEknIziowmFFcuRUYzSowmFJVXPi4zIb+0AnmlFcgvqbDfN1tu/p+oQi5Dl3ADbm/tj9tbB+DWaH94q2seZSGEwPkrpdiXmlu5XMGprJqHPxu0XogK0CEqQI/oAB1a+esQHahHlL8OBp0X1ErWOBGRczDc1IHhhpoSIQSKjCbk2cJO5a3JYoFFCFgsgEUI69UmhIBFAGYhIISAySxwPLMQu8/m4FxOicN+FXIZuoYbcHvrANze2h+B3mrsT83FvnNXsD81F5cKjNXK0jbYGx1CfXC5wIhzucU1bnMtL4UMOpW1GU5X2RynUymhV1fdBnqrEelvDUat/HUI9dXUex4hi0Ugo6AM57KLkZpTgnM5xcgqNMJHo4Sf1gsGnQotdNYmPkNlE5+f1trsp1TI7Z+v7bMtsIXL0qrPu6CsAi10Xuga7oduEQaEGTQN7pBORA3HcFMHhhtqji7mlWL3mZzKJRdpuSV1bq+Uy9A1woBbo/0RF9UCcdH+8L9m9FdJuQlpuSU4VxkqrLclSM0pxsW8UjS0wkmlkCOihRatAqoCT6S/DhovBdJyqkJMak4J0nJLUG5qWGdTvUqBMpPlhmvGAr3V6BZhQNdwg/U2woBgH02DyuBq+aUVOJdTjLPZ1vOTml0MixC4NcZag9c6UM+gRk0Gw00dGG6IgAt5pdh92hp2dp3JQX5pBXq2aoHboq1BpnuEH7SqhjcpmcwWlFSYUWI0o7jcZL8tNppQXG5tjisuN6PYaMKlgjKk5VqDyoUrpTDdYNhQymVo5a+zN4+F+GpQUm6t7corrUBeSTnySytwpaQceSUVKCyrfjFPlVJur9Gxdeb2rbzvq/FCZn4ZDl3Ix4lLhTWGoVBfDbpGGNAhxMf+Wl9N1f5sncT1KoVDmCgpNyG7sBzZxUbkFJUju8iInCIjsivvXykph1Iuh7daCZ3K1iG98lZV1Tldp1Yir6QcqdnWcJmaU4zU7GJcKal7uHawj7qy9i4A8W0CEB2gc0nYyS+pwJlsa9NmoLcaQT7qG+okn1dSjpNZRTiVVYSTl4pwMqsQp7OKkF1cji4tfXFbTAB6x/gjNroFfJvgNebKKqwzp3spZfZzq+As6NUw3NSB4Yao8TKZLcjIL0N6Zdg5V3mbnluCsgozWvlb+/ZEBVpvowP0CDNobmiovclsQUFl/yadSgGDtv6j0coqzDiaUYDD5/Nx6Hw+Dl/Iw8msomoTO9ZGKZfBV+sFrZcCucXlKK1w/VwlwT5qRAfoER1oDX/lJgv2nM3Br2l51Wq9QnytYSe+dQBio1rAT6eCXq2A1ktx3dBjtghcuFKK05eLKpdinL5chDOXix3mgrLxVisR6K1CoLfauvhY7wd4q2GxCGuQySrEqayaX18TmQzoGOqL22L8cVuMP26N9rdfesUZSspN9hBaVmGBVmX9bHQqBTReCvvja4NJhdmCjLwynL9SgvQrJTh/pRTpuZW3V0pqbOK1j6jUKOFjH1FpfRzso0FMoA4xgd6IDtQhyFvt8ho4Udn8LeWlZxhu6sBwQ0TOVGw04cjFAhw6n4e03BJ7fx3bUlB5a5sD6Vpqpbzyx12NQL0KAd5VP/L+ei+YzMJe41V8Vcf0kvKrOqUbTfDVeiEmQI+oQGvos41o09fScbyswoyUtDx77d3BtDyUm2tv4tNV9pey3irsNUkqhRwX8kpxJru4zibCEF81lHI5LhcZG9SUGO6nRZtgb7SrXNoGe8NPp8LB9DzsPZuDvWdzkZpTvbm1daAecdEt0EKvgpdcDoVcBqVcBoWi8lYur7y1Pi4ympBTXI6cosratKvu1zeMqpRye+gBgEsFZddtplUr5TBbxA3XXALWIBQdqEd0oB4xAXrEVN7303lZv4clFcgrLa+8rbhqnfV+YVkFTGaBCosFJrO1DCbzVfctFvv3V6WUV05hoYBepYSP5qrpLFTW8KVXK+Gv88KEvjE3/F7qwnBTB4YbInI3IQTKKiz2wFNaYUYLnRcCvNXVmqqkUlZhxq9pVyqbK3Nx5GI+istvrGZJpZAjJlCPNsF6tAnyRusg2623fXSeEAKFRhOyC6ua37KLjMguNOJy5WMAaBNUGWRCvNEmyLvWkHa1rIIy7E3Nxd6z1uX4pcJ616rVly2MarzkKKuwoLTCjNJy83WDj1pp7UsW0UKHSP/K2xY6RLTQItJfhxY6a3Oa0WSxj6osLLONrKxaCstMuJhXirPZ1r5UF/JKnf4enSHIR419Mwc5dZ8MN3VguCEiqh+LRaDMZEZJ+VX9p8rNKLnqtrTcgjCDBq2D9IhooWtUfUXySyqw/1wuDqbnobTcDJNF2GtHzBaL42OztYZCq1IiQK9CoLcKAd5qBOitt7bHtYVRIQSMJgtKKoNOabkZZRVmmC0CYX4aBOrVLmnSMZrMSM8twdlsa4fxsznFOHvZ2u+qyGhymBDU1v/L76q+ZQatF3w0XvBSyKGsrM1SyuXwUlhrs2zrFXIZFDIZSivMKDaa7WGr+OrbsqqpLbReCsy8t5NT3yvDTR0YboiIiJqeG/n95vzTRERE5FEYboiIiMijMNwQERGRR2G4ISIiIo/CcENEREQeheGGiIiIPArDDREREXkUhhsiIiLyKAw3RERE5FEYboiIiMijMNwQERGRR2G4ISIiIo/CcENEREQeheGGiIiIPIpS6gK4mxACgPXS6URERNQ02H63bb/jdWl24aawsBAAEBkZKXFJiIiI6EYVFhbCYDDUuY1M1CcCeRCLxYKLFy/Cx8cHMpnMqfsuKChAZGQk0tPT4evr69R9U8PxvDRePDeNE89L49Wcz40QAoWFhWjZsiXk8rp71TS7mhu5XI6IiAiXHsPX17fZfemaAp6XxovnpnHieWm8muu5uV6NjQ07FBMREZFHYbghIiIij8Jw40RqtRqzZs2CWq2Wuih0FZ6XxovnpnHieWm8eG7qp9l1KCYiIiLPxpobIiIi8igMN0RERORRGG6IiIjIozDcEBERkUdhuHGSxYsXIyYmBhqNBrGxsdixY4fURWp2fvrpJ9x3331o2bIlZDIZ1q5d6/C8EAKzZ89Gy5YtodVqMWDAABw5ckSawjYjiYmJuPXWW+Hj44Pg4GCMHDkSx48fd9iG50YaS5YsQbdu3ewTwsXHx+OHH36wP8/z0jgkJiZCJpNh2rRp9nU8N3VjuHGCpKQkTJs2DTNnzkRKSgr69euHoUOHIi0tTeqiNSvFxcXo3r073nvvvRqff/vtt7FgwQK899572LdvH0JDQzF48GD79cbINbZv346pU6di9+7dSE5OhslkQkJCAoqLi+3b8NxIIyIiAm+99Rb279+P/fv34+6778aIESPsP5I8L9Lbt28fli1bhm7dujms57m5DkE37bbbbhOTJ092WHfLLbeIv/3tbxKViACIb775xv7YYrGI0NBQ8dZbb9nXlZWVCYPBIJYuXSpBCZuvrKwsAUBs375dCMFz09i0aNFCfPDBBzwvjUBhYaFo166dSE5OFv379xfPP/+8EIL/ZuqDNTc3qby8HAcOHEBCQoLD+oSEBOzcuVOiUtG1zp49i8zMTIfzpFar0b9/f54nN8vPzwcA+Pv7A+C5aSzMZjO+/PJLFBcXIz4+nuelEZg6dSruvfdeDBo0yGE9z831NbsLZzpbdnY2zGYzQkJCHNaHhIQgMzNTolLRtWznoqbzdO7cOSmK1CwJITB9+nTccccd6NKlCwCeG6kdPnwY8fHxKCsrg7e3N7755ht06tTJ/iPJ8yKNL7/8Er/++iv27dtX7Tn+m7k+hhsnkclkDo+FENXWkfR4nqT17LPP4tChQ/j555+rPcdzI40OHTrg4MGDyMvLw+rVqzF+/Hhs377d/jzPi/ulp6fj+eefx6ZNm6DRaGrdjuemdmyWukmBgYFQKBTVammysrKqpWqSTmhoKADwPEnoueeew7p167B161ZERETY1/PcSEulUqFt27aIi4tDYmIiunfvjkWLFvG8SOjAgQPIyspCbGwslEollEoltm/fjn/9619QKpX2z5/npnYMNzdJpVIhNjYWycnJDuuTk5PRp08fiUpF14qJiUFoaKjDeSovL8f27dt5nlxMCIFnn30Wa9aswZYtWxATE+PwPM9N4yKEgNFo5HmR0MCBA3H48GEcPHjQvsTFxeHRRx/FwYMH0bp1a56b62CzlBNMnz4d48aNQ1xcHOLj47Fs2TKkpaVh8uTJUhetWSkqKsKpU6fsj8+ePYuDBw/C398frVq1wrRp0zBv3jy0a9cO7dq1w7x586DT6TB27FgJS+35pk6dis8//xzffvstfHx87P/bNBgM0Gq19vk7eG7c79VXX8XQoUMRGRmJwsJCfPnll9i2bRs2bNjA8yIhHx8fe580G71ej4CAAPt6npvrkG6glmd5//33RVRUlFCpVKJXr172Ya7kPlu3bhUAqi3jx48XQliHT86aNUuEhoYKtVot7rzzTnH48GFpC90M1HROAIjly5fbt+G5kcYTTzxh/7sVFBQkBg4cKDZt2mR/nuel8bh6KLgQPDfXIxNCCIlyFREREZHTsc8NEREReRSGGyIiIvIoDDdERETkURhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaImr1t27ZBJpMhLy9P6qIQkRMw3BAREZFHYbghIiIij8JwQ0SSE0Lg7bffRuvWraHVatG9e3esWrUKQFWT0ffff4/u3btDo9Ggd+/eOHz4sMM+Vq9ejc6dO0OtViM6OhrvvPOOw/NGoxGvvPIKIiMjoVar0a5dO3z44YcO2xw4cABxcXHQ6XTo06cPjh8/7to3TkQuwXBDRJL7+9//juXLl2PJkiU4cuQIXnjhBfz5z3/G9u3b7du8/PLLmD9/Pvbt24fg4GDcf//9qKioAGANJaNGjcIjjzyCw4cPY/bs2XjttdewYsUK++sfe+wxfPnll/jXv/6FY8eOYenSpfD29nYox8yZM/HOO+9g//79UCqVeOKJJ9zy/onIuXjhTCKSVHFxMQIDA7FlyxbEx8fb10+aNAklJSV46qmncNddd+HLL7/E6NGjAQC5ubmIiIjAihUrMGrUKDz66KO4fPkyNm3aZH/9K6+8gu+//x5HjhzBiRMn0KFDByQnJ2PQoEHVyrBt2zbcdddd2Lx5MwYOHAgAWL9+Pe69916UlpZCo9G4+FMgImdizQ0RSero0aMoKyvD4MGD4e3tbV9WrlyJ06dP27e7Ovj4+/ujQ4cOOHbsGADg2LFj6Nu3r8N++/bti5MnT8JsNuPgwYNQKBTo379/nWXp1q2b/X5YWBgAICsr66bfIxG5l1LqAhBR82axWAAA33//PcLDwx2eU6vVDgHnWjKZDIC1z47tvs3VldJarbZeZfHy8qq2b1v5iKjpYM0NEUmqU6dOUKvVSEtLQ9u2bR2WyMhI+3a7d++2379y5QpOnDiBW265xb6Pn3/+2WG/O3fuRPv27aFQKNC1a1dYLBaHPjxE5LlYc0NEkvLx8cFLL72EF154ARaLBXfccQcKCgqwc+dOeHt7IyoqCgAwd+5cBAQEICQkBDNnzkRgYCBGjhwJAHjxxRdx66234o033sDo0aOxa9cuvPfee1i8eDEAIDo6GuPHj8cTTzyBf/3rX+jevTvOnTuHrKwsjBo1Sqq3TkQuwnBDRJJ74403EBwcjMTERJw5cwZ+fn7o1asXXn31VXuz0FtvvYXnn38eJ0+eRPfu3bFu3TqoVCoAQK9evfDVV1/h9ddfxxtvvIGwsDDMnTsXEyZMsB9jyZIlePXVVzFlyhTk5OSgVatWePXVV6V4u0TkYhwtRUSNmm0k05UrV+Dn5yd1cYioCWCfGyIiIvIoDDdERETkUdgsRURERB6FNTdERETkURhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaIiIg8CsMNEREReRSGGyIiIvIoDDdERETkUf4fdFMhxmz2PKcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# Plot of loss vs epoch for train and test dataset\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Plot of loss vs epoch for train and test dataset\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
