{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfd468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968b5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('KAG_energydata_complete.csv',parse_dates=['date'], index_col= 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79fce30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:00:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.566667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.5300</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:10:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.992500</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:20:00</th>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>45.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:30:00</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.723333</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:40:00</th>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:20:00</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>46.560000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>42.025714</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>41.163333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>43.096812</td>\n",
       "      <td>43.096812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:30:00</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>25.754000</td>\n",
       "      <td>42.080000</td>\n",
       "      <td>27.133333</td>\n",
       "      <td>41.223333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>49.282940</td>\n",
       "      <td>49.282940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:40:00</th>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.596667</td>\n",
       "      <td>25.628571</td>\n",
       "      <td>42.768571</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>41.690000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.466667</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>29.199117</td>\n",
       "      <td>29.199117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:50:00</th>\n",
       "      <td>420</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.990000</td>\n",
       "      <td>25.414000</td>\n",
       "      <td>43.036000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>41.290000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8175</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>26.166667</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>6.322784</td>\n",
       "      <td>6.322784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 18:00:00</th>\n",
       "      <td>430</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>25.264286</td>\n",
       "      <td>42.971429</td>\n",
       "      <td>26.823333</td>\n",
       "      <td>41.156667</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.963333</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8450</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>34.118851</td>\n",
       "      <td>34.118851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Appliances  lights         T1       RH_1         T2  \\\n",
       "date                                                                       \n",
       "2016-01-11 17:00:00          60      30  19.890000  47.596667  19.200000   \n",
       "2016-01-11 17:10:00          60      30  19.890000  46.693333  19.200000   \n",
       "2016-01-11 17:20:00          50      30  19.890000  46.300000  19.200000   \n",
       "2016-01-11 17:30:00          50      40  19.890000  46.066667  19.200000   \n",
       "2016-01-11 17:40:00          60      40  19.890000  46.333333  19.200000   \n",
       "...                         ...     ...        ...        ...        ...   \n",
       "2016-05-27 17:20:00         100       0  25.566667  46.560000  25.890000   \n",
       "2016-05-27 17:30:00          90       0  25.500000  46.500000  25.754000   \n",
       "2016-05-27 17:40:00         270      10  25.500000  46.596667  25.628571   \n",
       "2016-05-27 17:50:00         420      10  25.500000  46.990000  25.414000   \n",
       "2016-05-27 18:00:00         430      10  25.500000  46.600000  25.264286   \n",
       "\n",
       "                          RH_2         T3       RH_3         T4       RH_4  \\\n",
       "date                                                                         \n",
       "2016-01-11 17:00:00  44.790000  19.790000  44.730000  19.000000  45.566667   \n",
       "2016-01-11 17:10:00  44.722500  19.790000  44.790000  19.000000  45.992500   \n",
       "2016-01-11 17:20:00  44.626667  19.790000  44.933333  18.926667  45.890000   \n",
       "2016-01-11 17:30:00  44.590000  19.790000  45.000000  18.890000  45.723333   \n",
       "2016-01-11 17:40:00  44.530000  19.790000  45.000000  18.890000  45.530000   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2016-05-27 17:20:00  42.025714  27.200000  41.163333  24.700000  45.590000   \n",
       "2016-05-27 17:30:00  42.080000  27.133333  41.223333  24.700000  45.590000   \n",
       "2016-05-27 17:40:00  42.768571  27.050000  41.690000  24.700000  45.730000   \n",
       "2016-05-27 17:50:00  43.036000  26.890000  41.290000  24.700000  45.790000   \n",
       "2016-05-27 18:00:00  42.971429  26.823333  41.156667  24.700000  45.963333   \n",
       "\n",
       "                     ...         T9     RH_9      T_out  Press_mm_hg  \\\n",
       "date                 ...                                               \n",
       "2016-01-11 17:00:00  ...  17.033333  45.5300   6.600000        733.5   \n",
       "2016-01-11 17:10:00  ...  17.066667  45.5600   6.483333        733.6   \n",
       "2016-01-11 17:20:00  ...  17.000000  45.5000   6.366667        733.7   \n",
       "2016-01-11 17:30:00  ...  17.000000  45.4000   6.250000        733.8   \n",
       "2016-01-11 17:40:00  ...  17.000000  45.4000   6.133333        733.9   \n",
       "...                  ...        ...      ...        ...          ...   \n",
       "2016-05-27 17:20:00  ...  23.200000  46.7900  22.733333        755.2   \n",
       "2016-05-27 17:30:00  ...  23.200000  46.7900  22.600000        755.2   \n",
       "2016-05-27 17:40:00  ...  23.200000  46.7900  22.466667        755.2   \n",
       "2016-05-27 17:50:00  ...  23.200000  46.8175  22.333333        755.2   \n",
       "2016-05-27 18:00:00  ...  23.200000  46.8450  22.200000        755.2   \n",
       "\n",
       "                        RH_out  Windspeed  Visibility  Tdewpoint        rv1  \\\n",
       "date                                                                          \n",
       "2016-01-11 17:00:00  92.000000   7.000000   63.000000   5.300000  13.275433   \n",
       "2016-01-11 17:10:00  92.000000   6.666667   59.166667   5.200000  18.606195   \n",
       "2016-01-11 17:20:00  92.000000   6.333333   55.333333   5.100000  28.642668   \n",
       "2016-01-11 17:30:00  92.000000   6.000000   51.500000   5.000000  45.410389   \n",
       "2016-01-11 17:40:00  92.000000   5.666667   47.666667   4.900000  10.084097   \n",
       "...                        ...        ...         ...        ...        ...   \n",
       "2016-05-27 17:20:00  55.666667   3.333333   23.666667  13.333333  43.096812   \n",
       "2016-05-27 17:30:00  56.000000   3.500000   24.500000  13.300000  49.282940   \n",
       "2016-05-27 17:40:00  56.333333   3.666667   25.333333  13.266667  29.199117   \n",
       "2016-05-27 17:50:00  56.666667   3.833333   26.166667  13.233333   6.322784   \n",
       "2016-05-27 18:00:00  57.000000   4.000000   27.000000  13.200000  34.118851   \n",
       "\n",
       "                           rv2  \n",
       "date                            \n",
       "2016-01-11 17:00:00  13.275433  \n",
       "2016-01-11 17:10:00  18.606195  \n",
       "2016-01-11 17:20:00  28.642668  \n",
       "2016-01-11 17:30:00  45.410389  \n",
       "2016-01-11 17:40:00  10.084097  \n",
       "...                        ...  \n",
       "2016-05-27 17:20:00  43.096812  \n",
       "2016-05-27 17:30:00  49.282940  \n",
       "2016-05-27 17:40:00  29.199117  \n",
       "2016-05-27 17:50:00   6.322784  \n",
       "2016-05-27 18:00:00  34.118851  \n",
       "\n",
       "[19735 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2d011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.resample('D').sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6d0a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.resample('D').mean()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae18b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    \n",
    "    # compute split point\n",
    "    end_idx = df.shape[0]* 70 // 100\n",
    "    \n",
    "    train_data = df.iloc[1:end_idx, : ]\n",
    "    test_data = df.iloc[end_idx:, :]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b715ba24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42, 28), (95, 28))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape , X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00520e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_data(train, test):\n",
    "    scaler = MinMaxScaler().fit(train)\n",
    "    return scaler.transform(train), scaler.transform(test), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c8beac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, scaler = scale_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3ddb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_splitter(df):\n",
    "\n",
    "    input_features = []\n",
    "    ouput_feature = []\n",
    "    \n",
    "    len_df = df.shape[0]\n",
    "    \n",
    "    for i in range(len_df):\n",
    "        \n",
    "        end_idx = i + 15\n",
    "        \n",
    "        if end_idx > len_df-15:\n",
    "            break\n",
    "            \n",
    "        input_x , output_y = df[i:end_idx, 1:], df[end_idx: end_idx+15, 0]\n",
    "        \n",
    "        input_features.append(input_x)\n",
    "        ouput_feature.append(output_y)\n",
    "    \n",
    "    return np.array(input_features), np.array(ouput_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef80267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = multivariate_splitter(X_train)\n",
    "X_test, Y_test = multivariate_splitter(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb15d2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of (training data) input features : (66, 15, 27) and ouput feature (66, 15)\n",
      "Shape of (testing data) input features : (13, 15, 27) and ouput feature (13, 15)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of (training data) input features : %s and ouput feature %s' % (X_train.shape, Y_train.shape))\n",
    "print('Shape of (testing data) input features : %s and ouput feature %s' % (X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0c858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dc29838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Energy_Models.RBFN import RBFN_Model\n",
    "RBFN_Model = BuildModel(n_steps,n_features,15).getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "839c9401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 15, 27)]          0         \n",
      "                                                                 \n",
      " build_model_1 (BuildModel)  (None, 15)                31324     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,324\n",
      "Trainable params: 31,270\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RBFN_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d362eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0419 - val_loss: 0.0735\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0418 - val_loss: 0.0734\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0418 - val_loss: 0.0734\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0417 - val_loss: 0.0733\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0417 - val_loss: 0.0733\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0417 - val_loss: 0.0732\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0416 - val_loss: 0.0732\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0416 - val_loss: 0.0731\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0415 - val_loss: 0.0731\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0415 - val_loss: 0.0730\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0414 - val_loss: 0.0730\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0414 - val_loss: 0.0729\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0413 - val_loss: 0.0729\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0413 - val_loss: 0.0728\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0412 - val_loss: 0.0728\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0412 - val_loss: 0.0727\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0411 - val_loss: 0.0727\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0411 - val_loss: 0.0726\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0410 - val_loss: 0.0726\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0410 - val_loss: 0.0725\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0410 - val_loss: 0.0725\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0409 - val_loss: 0.0724\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0409 - val_loss: 0.0724\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0408 - val_loss: 0.0723\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0408 - val_loss: 0.0723\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0407 - val_loss: 0.0722\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0407 - val_loss: 0.0722\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0406 - val_loss: 0.0721\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0406 - val_loss: 0.0721\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0406 - val_loss: 0.0720\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0405 - val_loss: 0.0720\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0405 - val_loss: 0.0719\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0404 - val_loss: 0.0719\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0404 - val_loss: 0.0718\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0403 - val_loss: 0.0718\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0403 - val_loss: 0.0718\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0403 - val_loss: 0.0717\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0402 - val_loss: 0.0717\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0402 - val_loss: 0.0716\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0401 - val_loss: 0.0716\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0401 - val_loss: 0.0715\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0400 - val_loss: 0.0715\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0400 - val_loss: 0.0714\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0400 - val_loss: 0.0714\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0399 - val_loss: 0.0714\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0399 - val_loss: 0.0713\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0398 - val_loss: 0.0713\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0398 - val_loss: 0.0712\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0398 - val_loss: 0.0712\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0397 - val_loss: 0.0711\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0397 - val_loss: 0.0711\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0396 - val_loss: 0.0710\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0396 - val_loss: 0.0710\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0395 - val_loss: 0.0710\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0395 - val_loss: 0.0709\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0395 - val_loss: 0.0709\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0394 - val_loss: 0.0708\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0394 - val_loss: 0.0708\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0394 - val_loss: 0.0707\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0393 - val_loss: 0.0707\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0393 - val_loss: 0.0707\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0392 - val_loss: 0.0706\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0392 - val_loss: 0.0706\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0392 - val_loss: 0.0705\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0391 - val_loss: 0.0705\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0391 - val_loss: 0.0705\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0390 - val_loss: 0.0704\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0390 - val_loss: 0.0704\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0390 - val_loss: 0.0703\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0389 - val_loss: 0.0703\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0389 - val_loss: 0.0703\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0389 - val_loss: 0.0702\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0388 - val_loss: 0.0702\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0388 - val_loss: 0.0701\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0388 - val_loss: 0.0701\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0387 - val_loss: 0.0701\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0387 - val_loss: 0.0700\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0386 - val_loss: 0.0700\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0386 - val_loss: 0.0699\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0386 - val_loss: 0.0699\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0385 - val_loss: 0.0699\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0385 - val_loss: 0.0698\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0385 - val_loss: 0.0698\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0384 - val_loss: 0.0698\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0384 - val_loss: 0.0697\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0384 - val_loss: 0.0697\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0383 - val_loss: 0.0696\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0383 - val_loss: 0.0696\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0383 - val_loss: 0.0696\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0382 - val_loss: 0.0695\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0382 - val_loss: 0.0695\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0382 - val_loss: 0.0695\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0381 - val_loss: 0.0694\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0381 - val_loss: 0.0694\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0381 - val_loss: 0.0694\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0380 - val_loss: 0.0693\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0380 - val_loss: 0.0693\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0380 - val_loss: 0.0693\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0379 - val_loss: 0.0692\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0379 - val_loss: 0.0692\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0379 - val_loss: 0.0691\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0378 - val_loss: 0.0691\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0378 - val_loss: 0.0691\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0378 - val_loss: 0.0690\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0377 - val_loss: 0.0690\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0377 - val_loss: 0.0690\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0377 - val_loss: 0.0689\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0376 - val_loss: 0.0689\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0376 - val_loss: 0.0689\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0376 - val_loss: 0.0688\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0375 - val_loss: 0.0688\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0375 - val_loss: 0.0688\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0375 - val_loss: 0.0687\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0375 - val_loss: 0.0687\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0374 - val_loss: 0.0687\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0374 - val_loss: 0.0686\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0374 - val_loss: 0.0686\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0373 - val_loss: 0.0686\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0373 - val_loss: 0.0685\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0373 - val_loss: 0.0685\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0372 - val_loss: 0.0685\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0372 - val_loss: 0.0685\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0372 - val_loss: 0.0684\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0372 - val_loss: 0.0684\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0371 - val_loss: 0.0684\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0371 - val_loss: 0.0683\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0371 - val_loss: 0.0683\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0370 - val_loss: 0.0683\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0370 - val_loss: 0.0682\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0370 - val_loss: 0.0682\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0370 - val_loss: 0.0682\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0369 - val_loss: 0.0681\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0369 - val_loss: 0.0681\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0369 - val_loss: 0.0681\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0369 - val_loss: 0.0681\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0368 - val_loss: 0.0680\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0368 - val_loss: 0.0680\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0368 - val_loss: 0.0680\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0367 - val_loss: 0.0679\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0367 - val_loss: 0.0679\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0367 - val_loss: 0.0679\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0367 - val_loss: 0.0678\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0366 - val_loss: 0.0678\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0366 - val_loss: 0.0678\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0366 - val_loss: 0.0678\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0366 - val_loss: 0.0677\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0365 - val_loss: 0.0677\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0365 - val_loss: 0.0677\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0365 - val_loss: 0.0677\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0365 - val_loss: 0.0676\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0364 - val_loss: 0.0676\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0364 - val_loss: 0.0676\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0364 - val_loss: 0.0675\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0364 - val_loss: 0.0675\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0363 - val_loss: 0.0675\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0363 - val_loss: 0.0675\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0363 - val_loss: 0.0674\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0363 - val_loss: 0.0674\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0362 - val_loss: 0.0674\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0362 - val_loss: 0.0674\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0362 - val_loss: 0.0673\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0362 - val_loss: 0.0673\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0361 - val_loss: 0.0673\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0361 - val_loss: 0.0673\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0361 - val_loss: 0.0672\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0361 - val_loss: 0.0672\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0360 - val_loss: 0.0672\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0360 - val_loss: 0.0671\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0360 - val_loss: 0.0671\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0360 - val_loss: 0.0671\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0360 - val_loss: 0.0671\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0359 - val_loss: 0.0671\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0359 - val_loss: 0.0670\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0359 - val_loss: 0.0670\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0359 - val_loss: 0.0670\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0358 - val_loss: 0.0670\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0358 - val_loss: 0.0669\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0358 - val_loss: 0.0669\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0358 - val_loss: 0.0669\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0358 - val_loss: 0.0669\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0357 - val_loss: 0.0668\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0357 - val_loss: 0.0668\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0357 - val_loss: 0.0668\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0357 - val_loss: 0.0668\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0357 - val_loss: 0.0667\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0356 - val_loss: 0.0667\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0356 - val_loss: 0.0667\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0356 - val_loss: 0.0667\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0356 - val_loss: 0.0666\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0355 - val_loss: 0.0666\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0355 - val_loss: 0.0666\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0355 - val_loss: 0.0666\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0355 - val_loss: 0.0666\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0355 - val_loss: 0.0665\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0354 - val_loss: 0.0665\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0354 - val_loss: 0.0665\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0354 - val_loss: 0.0665\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0354 - val_loss: 0.0664\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0354 - val_loss: 0.0664\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0353 - val_loss: 0.0664\n"
     ]
    }
   ],
   "source": [
    "RBFN_Model.compile(optimizer='adam', loss ='mse')\n",
    "history = RBFN_Model.fit(X_train , Y_train, epochs=200, batch_size=256, verbose= 1,validation_split=0.2,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfa53a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = RBFN_Model.predict(X_train)\n",
    "Y_pred_test  = RBFN_Model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15b3773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):    \n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "328c10e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE value for  RBFN Model  : 0.205 \n",
      "Train MSE value for  RBFN Model  : 0.042 \n",
      "Train R2 value for  RBFN Model  : -0.076 \n",
      "Train MAPE value for  RBFN Model  : 4146702530390.980 \n",
      "Train RMLSE value for  RBFN Model  : 0.020 \n",
      "Train MAE value for  RBFN Model  : 0.171 \n",
      "---------------------------------------------\n",
      "Test RMSE value for  RBFN Model  : 0.212 \n",
      "Test MSE value for  RBFN Model  : 0.045 \n",
      "Test R2 value for  RBFN Model  : -0.263 \n",
      "Test MAPE value for  RBFN Model  : 0.627 \n",
      "Test RMLSE value for  RBFN Model  : 0.022 \n",
      "Test MAE value for  RBFN Model  : 0.178 \n"
     ]
    }
   ],
   "source": [
    "print('Train RMSE value for  RBFN Model  : %.3f ' % root_mean_squared_error(Y_train, Y_pred_train))\n",
    "print('Train MSE value for  RBFN Model  : %.3f ' % mean_squared_error(Y_train, Y_pred_train))\n",
    "print('Train R2 value for  RBFN Model  : %.3f ' % r2_score(Y_train, Y_pred_train))\n",
    "print('Train MAPE value for  RBFN Model  : %.3f ' % mean_absolute_percentage_error(Y_train, Y_pred_train))\n",
    "print('Train RMLSE value for  RBFN Model  : %.3f ' % mean_squared_log_error(Y_train, Y_pred_train))\n",
    "print('Train MAE value for  RBFN Model  : %.3f ' % mean_absolute_error(Y_train, Y_pred_train))\n",
    "print('---------------------------------------------')\n",
    "print('Test RMSE value for  RBFN Model  : %.3f ' % root_mean_squared_error(Y_test, Y_pred_test))\n",
    "print('Test MSE value for  RBFN Model  : %.3f ' % mean_squared_error(Y_test, Y_pred_test))\n",
    "print('Test R2 value for  RBFN Model  : %.3f ' % r2_score(Y_test, Y_pred_test))\n",
    "print('Test MAPE value for  RBFN Model  : %.3f ' % mean_absolute_percentage_error(Y_test, Y_pred_test))\n",
    "print('Test RMLSE value for  RBFN Model  : %.3f ' % mean_squared_log_error(Y_test, Y_pred_test))\n",
    "print('Test MAE value for  RBFN Model  : %.3f ' % mean_absolute_error(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f92c8fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04192174971103668"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBFN_Model.evaluate(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fe761c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04495672136545181"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBFN_Model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6741a13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2pklEQVR4nO3deZxdVZ3v/c+35jE1JykyByKDQ0eMgANeFAcSaQNXpVFRRO8NXJt+7NvtAE+rrd23u2lvO/G6PtCoKLQCKjZtWlFQW+A6pEkFI4YwZCCQSoqkUklVUlMqqfo9f6x1UrtOnVM5J+RUVZLf+/Xar7OHtfdee5/hd9Zae68tM8M555zLVdFUZ8A559yJxQOHc865vHjgcM45lxcPHM455/LigcM551xePHA455zLiweOKSDpIUn/bZL29T8k7ZLUK6kpbdlCSSapZDLycqLK9/2a6JxPFUm3Svr0NMjHByX9aqrzkclkfi9PdB44CkTSNkkD8cdjl6RvSqrJcxsv6oddUinwReCtZlZjZl3Hsh2Xu0Kc8/hZevOL2YaZXWdmf/ti81JIx/OPjKRvSfpfxyNfWbb/ot+T6bSffHngKKw/NrMa4Fzg1cCnJnn/s4AK4IlJ3u+p7JjPuYK8v5NeYnSTzQPHJDCzHcBPgJelL5NUJOlTkp6TtFvSnZLq4uJH4mt3LLm8JsP65ZK+LGlnHL4c570EeDqx/n8cLZ+STpO0WtJeSZsl/ffEsvMktUnaH0tQX4zzKyR9W1KXpG5JayXNyrDtGyTdmzbvK5JujuMflLRV0gFJz0p6X5Y8FsVtbYn7/J6kxrgs9Y91VTwXHZL+8mjnKrF8paT18Ri3SLoksesFkn4d8/egpOYMect4ziW9Np6Xnvj62sQ6D0n6O0m/BvqBxWnb/BdgPvDv8TPwicRxfljS80BqP9+X9ELczyOSXprYzpF/4JIuktQu6S/jZ65D0jWZzndMf42kJ+Oxb5V0bWLZhNuS1BQ/U/slPQqcnm0/ZPm8S/pQ3P8+SQ9IWhDnS9KX4n57JD0u6WWSVgHvAz4Rt/PvWY7rLZKeiuv+H0CJZadL+o/4Gdsj6TuS6rO9Jzmc/xWSNsZzuEPSxxLLLo2fu25Jv5H0ion2My2YmQ8FGIBtwJvj+DzCP9C/jdMPAf8tjn8I2Ez4wagB/hX4l7hsIWBAyQT7+RtgDTATaAF+k9jPhOunLwceBv4/wj/mpUAncHFc9lvg/XG8Brggjl8L/DtQBRQDrwJmZNjXAsIP44w4XQx0ABcA1cB+4My4rBV4aZY8/3k83rlAOfDPwN1px3N33ObL4zG8OYdzdR7QA7yF8IdqDnBW4v3aArwEqIzTN+V4ThuBfcD7gRLgPXG6KbHt54GXxuWlE32W0vZxZzzOysRnqTaely8D6xPrfAv4X3H8IuBwPB+lwIr43jRkOaa3E37wBfyXmPbcXLYF3AN8L+bzZcAO4Fe5nLs47zLC9+PseH4+BfwmLnsbsA6oj3k7G2hNP94s+2omfObeFfP9P+NxpL6XZ8TPQjnhs/II8OVs70kO578DuDCONyTO37nAbuB8wnfi6rjt8mz7mQ7DlGfgZB3iG94LdAPPEX6QU1/whxIf0F8AH0msdyZwKH5Jxn2RMuxnC7AiMf02YFscn3D95HJCcBsGahPL/wH4Vhx/BPgc0Jy2jQ8RfoBfkcM5+RXwgTj+FmBLHK+O5+mdqXM0wTaeJAazON2a4XydlVj+eeAbOZyrfwa+lGWfDwGfSkx/BPjp0c5pnH4/8Ghamt8CH0xs+29y+CxlChyLJ1inPqapi9PfYmzgGGDsD/Ru4p+BHN7HfwM+erRtEX4ID6W9H39PfoHjJ8CHE9NFhMC0AHgT8EzcV1Hato4cb5Z9fQBYk5gW0E78XmZIfxnwu2zvSQ7n/3nCn6wZaeluIf55Scx7Gvgvuexnqgavqiqsy8ys3swWmNlHzGwgQ5rTCIEl5TnCj+C46p4sMq1/2jHk9TRgr5kdSNvWnDj+YcI/7qdidculcf6/AA8A98Tqn88rNBBnchfhHzfAe+M0ZtYH/AlwHdAh6ceSzsqyjQXAfbFY300IJMOMPV/b044hdT4mOlfzCIElmxcS4/2EUlcu0veZ2u+cxPR2js2R9SQVS7opVrHtJ/zgQPhnnUmXmR1OTGc9JknLJa1RqMLsJpQqktvNtq0Wwmc5/f3IxwLgK4n3ey/hR36Omf0H8H+ArwK7JN0maUaO2z0tmS8Lv9LJ8zlT0j2xWmk/8G2yn8tczv87CeftOUkPa7TaeQHwl6nji8c4j2P7Dk8aDxxTbyfhw5Myn1Bk3kX4x3Is6+88xnw0SqpN29YOADPbZGbvIVTz/CNwr6RqMztkZp8zs3OA1wKXEv7NZfJ94CJJc4HLiYEjbv8BM3sLoQTxFPC1LNvYDiyPATk1VFhoR0qZl3YMqfMx0bnazsT178cqfZ+p/Sbze7T3Odvy5Pz3AiuBNwN1hH/vkKi3PxYKbUA/AP4JmGVm9cD9OW63k/BZTn8/ssl0nNuBa9Pe70oz+w2Amd1sZq8iVPW9BPj4BNtK6kjmS5LS8vkPcRuvMLMZwFWMPeb07U94/s1srZmtJHx//o1QfZc6vr9LO74qM7s7x+OYEh44pt7dwP+UtEjhct2/B74b/8F1AiOkNZhmWP9TkloUGmw/Q/h3lBcz206ocvoHhQbvVxBKGd8BkHSVpBYzGyFUKwEMS3qjpJdLKibUGR8ilAAy7aOTUDXzTeBZM3sybnuWpHdIqgYOEqr4Mm4DuBX4u0QDaYuklWlpPi2pKjZOXgN8N86f6Fx9A7hG0sUKDfBzJij15ON+4CWS3iupRNKfAOcAP8pjG7uY+DMAoW79INBFaG/6+2PJbAZlhDr7TuCwpOXAW3NZ0cyGCW12n43vxzmEOvxsMn3ebwVuTDU0S6qT9O44/mpJ58cSbh8wyOjn5mjn7MfASyX9V4Wr0v4fYHZieS2xqlnSHEYDUkr69rOef0llkt4nqc7MDhG+J6l8fg24Lh6HJFVLenviD1wu7/3km+q6spN1YIK6Sca2cRQRfsC2E7443ybRSElodOwk/FiPq4MmNGTfTPgH1RHHK+KyheTXOD6X8IO2l1Btc10i7bcJdde9hIb+y+L89xDqZPsIH/Kbs+0vpn9/3OfHE/NaCQ3zPfE4HwLOybJ+EfAXcZ8HYj7/Pu14VhH+6b8AfCKXcxWXXw48Hre7GXhb+vsVpz9IfvX0ryc04vbE19dn+ixMcM5WEurIu4GPZdlHDfDDmPfnCKU+A86Iy7/F2DaO9jw+r38a39tuQtXkPblui1Bd9SPCj+WjwN9mO3fZPu/xM/OHuI3twO1x/sXx/eoF9hD+5NTEZUuA9XE7/5ZlX5cQ2kh6CFVeDzP6vXxpfK9643b+MnmcGd6TrOefEHx/SrgoYj+wNu0zcEmc1034XH6f2NaYvp+p/l1LDYqZc+6EJ2kh8CzhyqTDR0nunDtGXlXlnHMuLx44nHPO5cWrqpxzzuWloCUOSZdIelqh+4obMiyXpJvj8sclnRvnnxlvwU8N+yX9eVz22XhtdWrZikIeg3POubEK1jlavDzzq4Q7hNuBtZJWm9nGRLLlhKsflhBuub8FON/MniZ0eZHazg7gvsR6XzKzf8o1L83NzbZw4cJjPxjnnDsFrVu3bo+ZtaTPL2SvmucBm81sK4CkewiXliUDx0rgTgv1ZWsk1UtqNbOORJqLCV1T5HvH6RELFy6kra3tWFd3zrlTkqSMv7uFrKqaw9iuBtoZ281CrmmuJNy4lXR9rNq6XVJDpp0r9JDaJqmts7Mz/9w755zLqJCBI1OXBOkt8ROmkVQGvINwQ0zKLYSuIZYSbpb5Qqadm9ltZrbMzJa1tIwraTnnnDtGhQwc7Yzt+2Uu4/tQOlqa5cBjZrYrNcPMdpnZsIWuL75GqBJzzjk3SQrZxrEWWCJpEaFx+0pCR2BJqwnVTvcQGsd70to33kNaNVVaG8jlwIZCZN45d2o7dOgQ7e3tDA4OTnVWCq6iooK5c+dSWpqtY+uxChY4zOywpOsJXW4XE/qXeULSdXH5rYQO4FYQ+gXqJ3RIB4CkKsIVWdembfrzkpYSqrS2ZVjunHMvWnt7O7W1tSxcuJDQee7Jyczo6uqivb2dRYsW5bROQZ9VbGb3E4JDct6tiXEjdKCWad1+oCnD/Pcf52w659w4g4ODJ33QAJBEU1MT+VxE5F2OOOdcFid70EjJ9zgLWuI44T3zAOzeCDPPgZazoG4eFHmsdc6d2jxwTGTTz2Bt4kF0ZTXQcibMPBtazg6vM8+G2lY4Rf6ZOOcmT3d3N3fddRcf+chH8lpvxYoV3HXXXdTX1xckX6dEJ4fLli2zY75zfGAfdD4dSh67nwqvnU9BX6I+sKJubCBJBZYav3/EuRPVk08+ydlnnz2ledi2bRuXXnopGzaMvXh0eHiY4uLi47qvTMcraZ2ZLUtP6yWOo6lsgPkXhCGpbw/sfjIMnU+GoPLEfbDum6NpqpoTgeSsUOU186ywTeecO4obbriBLVu2sHTpUkpLS6mpqaG1tZX169ezceNGLrvsMrZv387g4CAf/ehHWbVqFTDazVJvby/Lly/n9a9/Pb/5zW+YM2cOP/zhD6msrHxR+fLAcayqm2HRhWFIMYPeXeNLJ+vvhqEDo+lqW2MgSZROZp4F5bXj9+Ocm3Kf+/cn2Lhz/3Hd5jmnzeCv//ilE6a56aab2LBhA+vXr+ehhx7i7W9/Oxs2bDhy2eztt99OY2MjAwMDvPrVr+ad73wnTU1jL0bdtGkTd999N1/72te44oor+MEPfsBVV131ovLugeN4kqB2dhhOf9PofDPoaU+UTuLQ9k04PDCarm5eaENpPhNaXhJfz4Sqxsk/FufctHPeeeeNudfi5ptv5r77Qsfh27dvZ9OmTeMCx6JFi1i6dCkAr3rVq9i2bduLzocHjskgQf28MLzkraPzR4ah+7nRQLL7SdjzNGz79diAUt2SFkzi64zTvFHeuUlwtJLBZKmurj4y/tBDD/Hzn/+c3/72t1RVVXHRRRdlvMu9vLz8yHhxcTEDAwPj0uTLA8dUKiqGxsVhOOvto/NHRqDneeh8JgSSzjhs+AEM9oymK6sdH0xazoSGhWHbzrkTWm1tLQcOHMi4rKenh4aGBqqqqnjqqadYs2bNpOXLA8d0VFQUfvwbFo4toZhB7+7RYLLnmfC69Zfw+7tG0xWXQdMZ0PyS0JaSCipNZ0BpxWQfjXPuGDU1NfG6172Ol73sZVRWVjJr1qwjyy655BJuvfVWXvGKV3DmmWdywQUXTLCl48svxz1ZDPbAnk2hMT4ZVLqfAxsJaVQE9fOhaQk0LwmBpOmMMO73ojg3xnS4HHcy+eW4p6KKOpi7LAxJhwaha3MspTwTAkrXJnju13CofzRdaTU0nR4DSgwqzTGw+NVezrkEDxwnu9IKmP2yMCSNjMCBjhBE9myCri1hvL0NNvwrY565VTN7fAml6QyoXwDF/hFy7lTj3/pTVVER1M0Jw+KLxi47NAj7no0BZXMssWyCjf8W7qQ/so1SaFwUSyjJ0srp4Uowr/py7qTkgcONV1oxenNiuv69MaBsGg0oXZth889geGg0XVlNCCqNi6Hx9NGrxxoXh/tcPKg4d8LywOHyU9UI888PQ9LIMHQ/H4LI3q2jwwsb4Kkfw8jh0bSlVTGIZAgsta3eA7Fz05wHDnd8FBXHQJDhCWLDh6Fn+9iAsndruOrrmQfGllRKKhMBZdHYoDJjjgcV56aBggYOSZcAXyE8OvbrZnZT2nLF5SsIj479oJk9JulM4LuJpIuBz5jZlyU1xmULCY+OvcLM9uGmr+KSRFC5eOyykeHQHUt6UOnaHLq1Hz6Y2E552EbDotH7XFJD/Xwoq5q0Q3JuMhxrt+oAX/7yl1m1ahVVVcf/e1Gw+zgkFQPPEJ4b3g6sBd5jZhsTaVYAf0YIHOcDXzGz8zNsZwdwvpk9J+nzwF4zu0nSDUCDmX1yorycEvdxnIxGRuDAzhhItiQCy7Ph/pSh3rHpa2ZlCCgLwqtXgbk8TYf7OLJ1q56LVA+5zc3NOaWfLvdxnAdsNrOtMQP3ACuBjYk0K4E747PH10iql9RqZh2JNBcDW8zsucQ6F8XxO4CHgAkDhztBFRVB3dwwLHrD2GVm0N8F+7aNH577Lfzh+6M3PkIordTPTwSVBWODS8WMSTkk5/KR7Fb9LW95CzNnzuR73/seBw8e5PLLL+dzn/scfX19XHHFFbS3tzM8PMynP/1pdu3axc6dO3njG99Ic3Mzv/zlL49rvgoZOOYA2xPT7YRSxdHSzAGSgeNK4O7E9KxUYDGzDkkzM+1c0ipgFcD8+fOPJf9uOpNC1/bVzeNvegQ4PBTaVVLBpPu50fHtj8LBnrHpq5rGBpL6+aND3VwofXHPL3AnuJ/cAC/84fhuc/bLYflNEyZJdqv+4IMPcu+99/Loo49iZrzjHe/gkUceobOzk9NOO40f//jHQOjDqq6uji9+8Yv88pe/zLnEkY9CBo5M11um14tNmEZSGfAO4MZ8d25mtwG3Qaiqynd9d4IrKQv3kzSdnnn5wL5EKSURVHY8Bht/OPYqMIDqmYlgMi8GlMR0WfX4fTh3HD344IM8+OCDvPKVrwSgt7eXTZs2ceGFF/Kxj32MT37yk1x66aVceOGFR9nSi1fIwNEOzEtMzwV25plmOfCYme1KzNuVqs6S1ArsPo55dqeKyoYwnPbK8cuGD4e76nu2h0uMu7eHEkvPduhYD0/9aOyVYBBKLHXzxpZU6uePzvOqsBPbUUoGk8HMuPHGG7n22mvHLVu3bh33338/N954I29961v5zGc+U9C8FDJwrAWWSFpEaNy+EnhvWprVwPWx/eN8oCetfeM9jK2mSq1zNXBTfP1hAfLuTmXFJaPPT1nw2vHLR0bCkx67n4/B5bkYXJ4PnUxuehAOpz0XoaJufCllxpxQDTZjTmjY98Z7lybZrfrb3vY2Pv3pT/O+972PmpoaduzYQWlpKYcPH6axsZGrrrqKmpoavvWtb41Z94SqqjKzw5KuBx4gXI57u5k9Iem6uPxW4H7CFVWbCZfjXpNaX1IV4Yqs9PB6E/A9SR8GngfeXahjcC6joiKY0RqGcc12hIb7vj0xsDyfKLU8H64Ke/bh8VeEFZXGbcaLAermjAaWVHCpbPA77k8xyW7Vly9fznvf+15e85rXAFBTU8O3v/1tNm/ezMc//nGKioooLS3llltuAWDVqlUsX76c1tbW49447t2qOzfZzGCwO9y/0rMD9rcnxneEUsz+Dhg5NHa90qpEMJkTg0yq1BLHva3luJkOl+NOpulyOa5zLhNptI1l9sszpxkZgb7dIZj0bI8BJTG+6clQXZZ+vUllw2gQmRE7saw9LZRmUq/eTb57kTxwODcdFRWFziBrZ8PcV2VOc3go3CCZLKkcGW+H59eEkk26stoYSFrDc+vHvMYAUzPTHz/ssvLA4dyJqqRs9N6TbIb64MALsH9nuFIs/fXZ/wu9L4y//FjFMXC1ji2tpL+e5FVjZoZOgXalfJssPHA4dzIrq574fhYI/YX17QkllTHBpSOUaDqfga0Pw8H949ctr4uBZHZ44FftrLTX2aH0cgJWj1VUVNDV1UVTU9NJHTzMjK6uLioqKnJexwOHc6e6ouLwQ187a+J0B3szl1r27wylmq5fh3aX9HtcIDyaeFxQSb7G8arGaXPl2Ny5c2lvb6ezs3Oqs1JwFRUVzJ07N+f0Hjicc7kpr4HyJeFJj9mYhbvye3eFYJLpteNx6P05DB0Yv35RaQgiWYPLrFCKqW6B4tLCHStQWlrKokUZHhPgPHA4544jKZQaqhozP0Ey6WBvCCSZgkvvrtiv2JrQmWUmlQ2hK5iambHfsplQ05KY1xKGmpne19hx5oHDOTc1ymvCMFH7C4Srx/p2w4FdoSH/wAuhTaZvN/Tuhr7OUIrp2zO+88qUstoQXFIBpWZmCDBH5iUCUPmMaVNdNl154HDOTW8lZaN30B/NocEQSPp2Q29n2ngMNF1b4PnfQv9exve7CpRUjC2tVDeHvsiqYm/MVXG6Os4rqz7lAo0HDufcyaO0YrSfsaMZPgz9e0ZLLX2dcTwRdPbvgI7fh9JM+p38KSUVMbA0pQWZpkSQSQScyoYTvl8yDxzOuVNTccnoTZZHYxYuR+7vgr6uEHD6u0JA6d8T58X5XVtCaSZT4z+AiqCyMRFk0gNOc2gjqoxtRVVNobuZaVSq8cDhnHNHI4UejivqoHFxbuscGozBpCsRXNICTv9e6Hwanvt19qozCE+wTAaTyoYYaJLzEoGmsgEq6gtWsvHA4ZxzhVBaETuhnJNb+pFhGOgeDS79e2Fgb9r4vvDa+XSc3gs2nHl7KgrB4123w+lvPF5HBXjgcM656aGoOLSLVDflvk6yCi0VVI4Emfg6I8fAlQcPHM45d6IaU4U2ebs9sZv2nXPOTToPHM455/JS0MAh6RJJT0vaLOmGDMsl6ea4/HFJ5yaW1Uu6V9JTkp6U9Jo4/7OSdkhaH4cVhTwG55xzYxWsjUNSMfBVwnPD24G1klab2cZEsuXAkjicD9zC6EOcvwL81MzeJakMqEqs9yUz+6dC5d0551x2hSxxnAdsNrOtZjYE3AOsTEuzErjTgjVAvaRWSTOANwDfADCzITPrLmBenXPO5aiQgWMOsD0x3R7n5ZJmMdAJfFPS7yR9XVLyUWPXx6qt2yU1ZNq5pFWS2iS1nQr96Tvn3GQpZODIdH98+m2R2dKUAOcCt5jZK4E+INVGcgtwOrAU6AC+kGnnZnabmS0zs2UtLS35594551xGhQwc7UCyp7G5wM4c07QD7Wb2n3H+vYRAgpntMrNhMxsBvkaoEnPOOTdJChk41gJLJC2KjdtXAqvT0qwGPhCvrroA6DGzDjN7Adgu6cyY7mJgI4Ck1sT6lwMbCngMzjnn0hTsqiozOyzpeuABoBi43cyekHRdXH4rcD+wAtgM9APXJDbxZ8B3YtDZmlj2eUlLCVVa24BrC3UMzjnnxpNZlt4YTyLLli2ztra2qc6Gc86dUCStM7Nl6fP9znHnnHN58cDhnHMuLx44nHPO5cUDh3POubx44HDOOZcXDxzOOefy4oHDOedcXjxwOOecy4sHDuecc3nxwOGccy4vHjicc87lxQOHc865vHjgcM45lxcPHM455/LigcM551xePHA455zLiwcO55xzeSlo4JB0iaSnJW2WdEOG5ZJ0c1z+uKRzE8vqJd0r6SlJT0p6TZzfKOlnkjbF14ZCHoNzzrmxChY4JBUDXwWWA+cA75F0Tlqy5cCSOKwCbkks+wrwUzM7C/gj4Mk4/wbgF2a2BPhFnHbOOTdJClniOA/YbGZbzWwIuAdYmZZmJXCnBWuAekmtkmYAbwC+AWBmQ2bWnVjnjjh+B3BZAY/BOedcmkIGjjnA9sR0e5yXS5rFQCfwTUm/k/R1SdUxzSwz6wCIrzMz7VzSKkltkto6Oztf/NE455wDChs4lGGe5ZimBDgXuMXMXgn0kWeVlJndZmbLzGxZS0tLPqs655ybQCEDRzswLzE9F9iZY5p2oN3M/jPOv5cQSAB2SWoFiK+7j3O+nXPOTaCQgWMtsETSIkllwJXA6rQ0q4EPxKurLgB6zKzDzF4Atks6M6a7GNiYWOfqOH418MMCHoNzzrk0JYXasJkdlnQ98ABQDNxuZk9Iui4uvxW4H1gBbAb6gWsSm/gz4Dsx6GxNLLsJ+J6kDwPPA+8u1DE455wbT2bpzQ4nn2XLlllbW9tUZ8M5504oktaZ2bL0+X7nuHPOubx44HDOOZcXDxzOOefy4oHDOedcXjxwOOecy4sHDuecc3nxwOGccy4vHjicc87lxQOHc865vHjgcM45lxcPHM455/LigcM551xePHA455zLiwcO55xzefHA4ZxzLi8eOJxzzuXFA4dzzrm85BQ4JH1U0oz4bPBvSHpM0ltzWO8SSU9L2izphgzLJenmuPxxSecmlm2T9AdJ6yW1JeZ/VtKOOH+9pBW5HqxzzrkXL9cSx4fMbD/wVqCF8PzvmyZaQVIx8FVgOXAO8B5J56QlWw4sicMq4Ja05W80s6UZHl34pTh/qZndn+MxOOecOw5yDRyKryuAb5rZ7xPzsjkP2GxmW81sCLgHWJmWZiVwpwVrgHpJrTnmyTnn3BTINXCsk/QgIXA8IKkWGDnKOnOA7Ynp9jgv1zQGPChpnaRVaetdH6u2bpfUkGnnklZJapPU1tnZeZSsOuecy1WugePDwA3Aq82sHyglVFdNJFOJxPJI8zozO5dQnfWnkt4Q598CnA4sBTqAL2TauZndZmbLzGxZS0vLUbLqnHMuV7kGjtcAT5tZt6SrgE8BPUdZpx2Yl5ieC+zMNY2ZpV53A/cRqr4ws11mNmxmI8DXUvOdc85NjlwDxy1Av6Q/Aj4BPAfceZR11gJLJC2SVAZcCaxOS7Ma+EC8uuoCoMfMOiRVx+owJFUTGuU3xOlkG8jlqfnOOecmR0mO6Q6bmUlaCXzFzL4h6eqJVjCzw5KuBx4AioHbzewJSdfF5bcC9xPaTTYD/YxWf80C7pOUyuNdZvbTuOzzkpYSqrS2AdfmeAzOOeeOA5mlNztkSCQ9DPwU+BBwIdAJrDezlxc2e8fHsmXLrK2t7egJnXPOHSFpXYbbIXKuqvoT4CDhfo4XCFc+/e/jmD/nnHMniJwCRwwW3wHqJF0KDJrZ0do4nHPOnYRy7XLkCuBR4N3AFcB/SnpXITPmnHNuesq1cfyvCPdw7AaQ1AL8HLi3UBlzzjk3PeXaxlGUChpRVx7rOuecO4nkWuL4qaQHgLvj9J8QLqV1zjl3iskpcJjZxyW9E3gdoZuQ28zsvoLmzDnn3LSUa4kDM/sB8IMC5sU559wJYMLAIekA4zsmhFDqMDObUZBcOeecm7YmDBxmVjtZGXHOOXdi8CujnHPO5cUDh3POubx44HDOOZcXDxzOOefy4oHDOedcXjxwOOecy4sHDuecc3kpaOCQdImkpyVtlnRDhuWSdHNc/rikcxPLtkn6g6T1ktoS8xsl/UzSpvjaUMhjcM45N1bBAoekYuCrwHLgHOA9ks5JS7YcWBKHVcAtacvfaGZL0x5deAPwCzNbAvwiTjvnnJskhSxxnAdsNrOtZjYE3AOsTEuzErjTgjVAvaTWo2x3JXBHHL8DuOw45tk559xRFDJwzAG2J6bb47xc0xjwoKR1klYl0swysw6A+Doz084lrZLUJqmts7PzRRyGc865pEIGDmWYl95h4kRpXmdm5xKqs/5U0hvy2bmZ3WZmy8xsWUtLSz6rOuecm0AhA0c7MC8xPRfYmWsaM0u97gbuI1R9AexKVWfF1+STCZ1zzhVYIQPHWmCJpEWSyoArgdVpaVYDH4hXV10A9JhZh6RqSbUAkqqBtwIbEutcHcevBn5YwGNwzjmXJucHOeXLzA5Luh54ACgGbjezJyRdF5ffSnj87ApgM9APXBNXnwXcJymVx7vM7Kdx2U3A9yR9GHgeeHehjsE559x4Msv0nKaTy7Jly6ytre3oCZ1zzh0haV3a7RCA3znunHMuTx44nHPO5cUDh3POubx44HDOOZcXDxzOOefy4oHDOedcXjxwOOecy4sHDuecc3nxwOGccy4vHjicc87lxQOHc865vHjgcM45lxcPHM455/LigcM551xePHA455zLiwcO55xzefHA4ZxzLi8FDRySLpH0tKTNkm7IsFySbo7LH5d0btryYkm/k/SjxLzPStohaX0cVhTyGJxzzo1VsGeOSyoGvgq8BWgH1kpabWYbE8mWA0vicD5wS3xN+SjwJDAjbfNfMrN/KlTenXPOZVfIEsd5wGYz22pmQ8A9wMq0NCuBOy1YA9RLagWQNBd4O/D1AubROedcngoZOOYA2xPT7XFermm+DHwCGMmw7etj1dbtkhoy7VzSKkltkto6OzuPJf/OOecyKGTgUIZ5lksaSZcCu81sXYbltwCnA0uBDuALmXZuZreZ2TIzW9bS0pJ7rp1zzk2okIGjHZiXmJ4L7MwxzeuAd0jaRqjiepOkbwOY2S4zGzazEeBrhCox55xzk6SQgWMtsETSIkllwJXA6rQ0q4EPxKurLgB6zKzDzG40s7lmtjCu9x9mdhVAqg0kuhzYUMBjcM45l6ZgV1WZ2WFJ1wMPAMXA7Wb2hKTr4vJbgfuBFcBmoB+4JodNf17SUkK11zbg2uOfe+ecc9nILL3Z4eSzbNkya2trm+psOOfcCUXSOjNblj7f7xx3zjmXFw8czjnn8uKBwznnXF48cDjnnMuLBw7nnHN58cDhnHMuLx44nHPO5cUDh3POubx44HDOOZeXgnU5cjK47ZEtPPR0J2fMrAlDSw2nz6xhZm05UqaOfZ1z7uTngWMC5SXF9A0N86+P7aD34OEj82srSji9pWZMQDljZg3zGqsoLvKA4pw7uXlfVTkwM3YfOMjm3b1jh85eOg8cPJKurKSIxc3VnB5LJqmgsrilmorS4uNxKM45N2my9VXlJY4cSGLWjApmzajgdWc0j1nWM3CILZ0hkGyJAWXDzh5+sqGDEUutD/MaqjhjZg2nt1QnSiq11FWVTsEROefcsfPA8SLVVZZy7vwGzp0/9gm2g4eG2dbVN66U8qvNexg6PPo03Oaacs6YWX2k6uv0lhoWNVczp76SIq/2cs5NQx44CqSitJizZs/grNkzxswfHjHa9/UfKaWkhn///U72D462o5SXFLGwqZrFLWFY1ByqvBY3V1NfVTbZh+Occ0d44JhkxUViQVM1C5qqedNZs47MNzM6ew/ybGcfW/f08eyePrZ29vL0rgP8bOMuDo+MtkU1VpexqDkEkUUt1SyOQWVBUxXlJd6W4pwrLA8c04QkZtZWMLO2gvMXN41Zdmh4hO17+2MwCYFla2cvDz/TyffXtR9JVySY01B5JJAsbq5mcaz6mj2jwqu+nHPHRUEDh6RLgK8QHh37dTO7KW254vIVhEfHftDMHkssLwbagB1mdmmc1wh8F1hIeHTsFWa2r5DHMdVKi4tY3FLD4pYaLj577LIDg4d4NpZQtnSOllTWbttL/9DwkXSVpcUsjKWUhc1VLGiqZmFTNQubqmjx+1Kcc3koWOCIP/pfBd4CtANrJa02s42JZMuBJXE4H7glvqZ8FHgSSDYU3AD8wsxuknRDnP5koY5juqutKOUVc+t5xdz6MfPNjF37D7J1T28opXT28eyeXp7Y2cNPn3iB4UTVV1VZcQwkVWNfm6uYVeslFefcWIUscZwHbDazrQCS7gFWAsnAsRK408LNJGsk1UtqNbMOSXOBtwN/B/xF2joXxfE7gIc4hQNHNpKYXVfB7LoKXnv62EuIDw2PsLN7gG1d/TzX1ce2Pf1s6+rj6V0H+PmTuzg0PBpUKkqLWNAY2k8WNofXRU3VLGiuptWrv5w7JRUycMwBtiem2xlbmsiWZg7QAXwZ+ARQm7bOLDPrAIgBZmamnUtaBawCmD9//rEdwUmqtLjoSAM9tIxZNjxi7Owe4Lmufp7t6uO5PX1s6wrtKw890znmUuKykiLmN1YlSiihtDK/sYrT6ispLfau0Jw7GRUycGT6K5p+m3rGNJIuBXab2TpJFx3Lzs3sNuA2CHeOH8s2TkXFRWJeYxXzGqt4/ZKxJZWREaNj/+CRYPJcVx/bYonlV5v3MHhoNKgUCU6rr2ReQwgk85vCNuc3VjGvoZLG6jJvV3HuBFXIwNEOzEtMzwV25pjmXcA7JK0AKoAZkr5tZlcBuxLVWa3A7oIdgRujqEjMqa9kTn0lrz1j7LKRkdAty7N7+ti+r5/te/t5fm94/cVTu9nTe3BM+uqy4iOB5EhgaQjBZW5DpXfR4tw0VsjAsRZYImkRsAO4EnhvWprVwPWx/eN8oCdWQ90YB2KJ42MxaKTWuRq4Kb7+sIDH4HJUVDTapvIamsYt7x86zPa9A0eCSer12T19PLKpc0xpBWD2jArmN1Yxt7FyNLjEwa8Cc25qFSxwmNlhSdcDDxAux73dzJ6QdF1cfitwP+FS3M2Ey3GvyWHTNwHfk/Rh4Hng3YXIvzu+qspKOHN2LWfOTm+yGr35MRVQnu8aYPu+MP7bLV3c97sdJPviLC8pCiWfhkrmNlQyt6GKOfVhfE5DJTNrK7yXYucKyHvHddPe4KFhdnQPsD2WUrbvG2DHvgHa9/Wzo3uAPb1DY9KXFovWuhhI6mNgaRidbq2roMQb7p07Ku8d152wKkqLQ1f1LTUZlw8MhcCSCiTticDyyKZOdu0f275SXCRmz6gIwSRRUkmVXFrrK7zrFucm4IHDnfAqy4qPdFWfycHDw3R0D4aA0t2fCCwD/Oeze/m39QMk7odECr0Wn1ZXwWn1lbTWVXJafQWtdSGozKmvpLmm3KvD3CnLA4c76ZWXhO5WFjZXZ1x+aHiEF3pSgSWUVDq6B9nZM8Azuw7w8DOdY7pvASgpCs9oSQWU0+oTwaUuBJf6qlJvxHcnJQ8c7pRXWlx05N6VTMyM/QOH2dkzQEfPADu6B+noHqCjZ5Cd3QOs397NTze8wNDw2CvDKkqLOC2WUlrrKkdLMPVhvLW+kppy/wq6E49/ap07CknUVZVSV1XK2a0zMqYZGTH29B2ko3uQjp4BdiZed/YM8KtNe9h9YHBMlRhATXkJs2aUM7suPGFy9oyKceNeLeamGw8czh0HRUWj3eL/0bz6jGkODY+w+8BBOrpDlVhHzyAv9Ayya/8gL+wfZM2WLnYfODjm2SsQGvNbasqZVVfB7BnlzJ5Rway6CmbVJoJMXYWXXtyk8U+ac5OktLjoyJ33465vjFIll109B3khBpTd+0OAeWH/IM/u6eO3W7rGPC0yZaLSy8zacmbOqKClppyyEr8U2b04Hjicm0aSJZeXU5c1Xf/QYXbtPzimxJJL6QWgvqo0BJLaEFBaZpTTUhMCS5gfxr0E47LxT4ZzJ6CqshIWNZewKMuVYjC29NLZO8ju/QfZfeAguw+Mjj+7p4/OAwfHNeyHfRTTkgoktRVhPEOQaagq8+71TzEeOJw7SSVLL0xQejEzegYOhaCyPwaWAwfpPBADzf5BnuzYz8PPHKT34PgqspIi0VwTgkpzTTnNNWXxtZymmjJaaspprg3T9ZWlHmROAh44nDvFSaK+qoz6qjJeMmt8X2JJ/UOHEwFlNMikxl/oGWTDjh66+obGPGUypbhINFaXpQWYsYGmuaacltpyGqvL/Jku05QHDudczqrKSljQVBIfApbdyEgoxezpPUhn70H29A7R1XuQPb0H2XNgKLz2DbG1s489vQc5eHh8VRmE9phUcGmqCdVkqUDTVFNOY3UpjdUhyMyoKPEbLieJBw7n3HFXVCQaqstoqC5jyVFKMWZG39Awew7EwBIDzZ5EoOnqO8jGnfvZc+AgBzJUl0GoMmuoLqOpuozGuO/UeFOcDuPlNFSX0lhV5p1dHiMPHM65KSWJmvISaspLsnYLkzR4aJiuvlCC6eobYl/fEHv7hujqG2Jv7xB7+8P0xp372ds3RM/AoazbqqssTQsqZVmDT0NVGVVlxV6qwQOHc+4EU1FafOR+mFwcGh5hXwwmmYZU8Nm+t5/127vZ1zeU8TJmgLLiIuqrSuNQRkNVKQ2xfai+qpSGI/PDsrq4/GRrq/HA4Zw7qZUWFyWuLjs6M2P/4OEYWA6yt+8Qe/sOsq//EPv6h+juO0T3wBD7+g/x7J4+Huvvprt/iEPD2Z9tVFNeEgNL2ZigU19VRn1lKQ3VYwNOfWUZtRUl0/YKNA8czjmXIIm6ylLqKksnvE8myczoHxoOgSUVYPoP0d0/NBpwEtPb9/azr/8Q+wcPke1ZesVFYkZFSchLVdmRPNVVllBfmZiuKj0yXh/HK0sLW6VW0MAh6RLgK4RHx37dzG5KW664fAXh0bEfNLPHJFUAjwDlMY/3mtlfx3U+C/x3oDNu5v81s/sLeRzOOTcRSVSXl1BdXsLchtzXGx4x9g+EwLIvBpZk4OkZOET3QHjtGTjE8119R8az1KYB4SmYqWDy95e/nPMXN734g0woWOCQVAx8FXgL0A6slbTazDYmki0HlsThfOCW+HoQeJOZ9UoqBX4l6Sdmtiau9yUz+6dC5d055yZDceLqs3yMjBi9Q4fpicFlf1qA6U7Mr60oPe75LmSJ4zxgs5ltBZB0D7ASSAaOlcCdFh58vkZSvaRWM+sAemOa0jic/A9Hd865HBQViRkVpcyoKGXeVOy/gNueA2xPTLfHeTmlkVQsaT2wG/iZmf1nIt31kh6XdLukjAVDSasktUlq6+zszJTEOefcMShk4MjUMpNeasiaxsyGzWwpMBc4T9LL4vJbgNOBpUAH8IVMOzez28xsmZkta2lpyT/3zjnnMipk4GiHMaWoucDOfNOYWTfwEHBJnN4Vg8oI8DVClZhzzrlJUsjAsRZYImmRpDLgSmB1WprVwAcUXAD0mFmHpBZJ9QCSKoE3A0/F6dbE+pcDGwp4DM4559IUrHHczA5Luh54gHA57u1m9oSk6+LyW4H7CZfibiZcjntNXL0VuCNemVUEfM/MfhSXfV7SUkKV1jbg2kIdg3POufFk2e4+OYksW7bM2trapjobzjl3QpG0zszGPen45OpAxTnnXMF54HDOOZeXU6KqSlIn8Nwxrt4M7DmO2Tlepmu+YPrmzfOVn+maL5i+eTvZ8rXAzMbdz3BKBI4XQ1Jbpjq+qTZd8wXTN2+er/xM13zB9M3bqZIvr6pyzjmXFw8czjnn8uKB4+hum+oMZDFd8wXTN2+er/xM13zB9M3bKZEvb+NwzjmXFy9xOOecy4sHDuecc3nxwDEBSZdIelrSZkk3TGE+5kn6paQnJT0h6aNx/mcl7ZC0Pg4rpiBv2yT9Ie6/Lc5rlPQzSZviax4P0zwueTozcU7WS9ov6c+n6nzF58bslrQhMS/rOZJ0Y/zMPS3pbZOcr/8t6an4vJv7Ep2NLpQ0kDh3t05yvrK+d1N8vr6byNO2+AyhyT5f2X4fCvcZMzMfMgyEjhm3AIuBMuD3wDlTlJdW4Nw4Xgs8A5wDfBb42BSfp21Ac9q8zwM3xPEbgH+c4vfxBWDBVJ0v4A3AucCGo52j+L7+HigHFsXPYPEk5uutQEkc/8dEvhYm003B+cr43k31+Upb/gXgM1NwvrL9PhTsM+YljuyOPPrWzIaA1KNvJ52ZdZjZY3H8APAk45+mOJ2sBO6I43cAl01dVrgY2GJmx9pzwItmZo8Ae9NmZztHK4F7zOygmT1L6Dm6IM+cyZQvM3vQzA7HyTWEZ+RMqiznK5spPV8pkgRcAdxdiH1PZILfh4J9xjxwZJfLo28nnaSFwCuB1KN0j/oY3QIz4EFJ6yStivNmWXhuPPF15hTkK+VKxn6Zp/p8pWQ7R9Ppc/ch4CeJ6UWSfifpYUkXTkF+Mr130+V8XQjsMrNNiXmTfr7Sfh8K9hnzwJFdLo++nVSSaoAfAH9uZvvJ8TG6BfY6MzsXWA78qaQ3TEEeMlJ4gNg7gO/HWdPhfB3NtPjcSfor4DDwnTirA5hvZq8E/gK4S9KMScxStvduWpwv4D2M/YMy6ecrw+9D1qQZ5uV1zjxwZJfLo28njaRSwofiO2b2rzA9HqNrZjvj627gvpiHXYpPaoyvuyc7X9Fy4DEz2xXzOOXnKyHbOZryz52kq4FLgfdZrBSP1RpdcXwdoV78JZOVpwneu+lwvkqA/wp8NzVvss9Xpt8HCvgZ88CRXS6Pvp0Usf70G8CTZvbFxPwpfYyupGpJtalxQsPqBsJ5ujomuxr44WTmK2HMv8CpPl9psp2j1cCVksolLQKWAI9OVqYkXQJ8EniHmfUn5rcoPJETSYtjvrZOYr6yvXdTer6iNwNPmVl7asZknq9svw8U8jM2Ga3+J+pAeKztM4R/C381hfl4PaEo+TiwPg4rgH8B/hDnrwZaJzlfiwlXZ/weeCJ1joAm4BfApvjaOAXnrAroAuoS86bkfBGCVwdwiPBv78MTnSPgr+Jn7mlg+STnazOh/jv1Obs1pn1nfI9/DzwG/PEk5yvrezeV5yvO/xZwXVrayTxf2X4fCvYZ8y5HnHPO5cWrqpxzzuXFA4dzzrm8eOBwzjmXFw8czjnn8uKBwznnXF48cDg3zUm6SNKPpjofzqV44HDOOZcXDxzOHSeSrpL0aHz+wj9LKpbUK+kLkh6T9AtJLTHtUklrNPrci4Y4/wxJP5f0+7jO6XHzNZLuVXhWxnfi3cLOTQkPHM4dB5LOBv6E0OnjUmAYeB9QTegv61zgYeCv4yp3Ap80s1cQ7ohOzf8O8FUz+yPgtYQ7lSH0ePrnhGcpLAZeV+BDci6rkqnOgHMniYuBVwFrY2GgktCp3Aijnd99G/hXSXVAvZk9HOffAXw/9vs1x8zuAzCzQYC4vUct9oUUnzK3EPhVwY/KuQw8cDh3fAi4w8xuHDNT+nRauon6+Jmo+ulgYnwY/+66KeRVVc4dH78A3iVpJhx53vMCwnfsXTHNe4FfmVkPsC/xcJ/3Aw9beIZCu6TL4jbKJVVN5kE4lwv/1+LccWBmGyV9ivA0xCJCD6p/CvQBL5W0DughtINA6Ob61hgYtgLXxPnvB/5Z0t/Ebbx7Eg/DuZx477jOFZCkXjOrmep8OHc8eVWVc865vHiJwznnXF68xOGccy4vHjicc87lxQOHc865vHjgcM45lxcPHM455/Ly/wNvUIbJsufkGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of loss vs epoch for train and test dataset\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Plot of loss vs epoch for train and test dataset\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
