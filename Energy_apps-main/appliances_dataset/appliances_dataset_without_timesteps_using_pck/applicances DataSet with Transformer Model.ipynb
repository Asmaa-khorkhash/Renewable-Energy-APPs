{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc894356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.261788Z",
     "iopub.status.busy": "2023-06-24T17:39:39.261328Z",
     "iopub.status.idle": "2023-06-24T17:39:39.268561Z",
     "shell.execute_reply": "2023-06-24T17:39:39.267251Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.261751Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Energy_Models.Transformer import Transformer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a7d6e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.367347Z",
     "iopub.status.busy": "2023-06-24T17:39:39.366856Z",
     "iopub.status.idle": "2023-06-24T17:39:39.579229Z",
     "shell.execute_reply": "2023-06-24T17:39:39.578100Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.367313Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('../input/appliances-energy-prediction/KAG_energydata_complete.csv' ,index_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4baa176e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.581562Z",
     "iopub.status.busy": "2023-06-24T17:39:39.581199Z",
     "iopub.status.idle": "2023-06-24T17:39:39.629707Z",
     "shell.execute_reply": "2023-06-24T17:39:39.628577Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.581533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:00:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.566667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.5300</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:10:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.992500</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:20:00</th>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>45.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:30:00</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.723333</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:40:00</th>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:20:00</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>46.560000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>42.025714</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>41.163333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>43.096812</td>\n",
       "      <td>43.096812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:30:00</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>25.754000</td>\n",
       "      <td>42.080000</td>\n",
       "      <td>27.133333</td>\n",
       "      <td>41.223333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>49.282940</td>\n",
       "      <td>49.282940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:40:00</th>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.596667</td>\n",
       "      <td>25.628571</td>\n",
       "      <td>42.768571</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>41.690000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.466667</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>29.199117</td>\n",
       "      <td>29.199117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 17:50:00</th>\n",
       "      <td>420</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.990000</td>\n",
       "      <td>25.414000</td>\n",
       "      <td>43.036000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>41.290000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8175</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>26.166667</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>6.322784</td>\n",
       "      <td>6.322784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 18:00:00</th>\n",
       "      <td>430</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>25.264286</td>\n",
       "      <td>42.971429</td>\n",
       "      <td>26.823333</td>\n",
       "      <td>41.156667</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.963333</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8450</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>34.118851</td>\n",
       "      <td>34.118851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Appliances  lights         T1       RH_1         T2  \\\n",
       "date                                                                       \n",
       "2016-01-11 17:00:00          60      30  19.890000  47.596667  19.200000   \n",
       "2016-01-11 17:10:00          60      30  19.890000  46.693333  19.200000   \n",
       "2016-01-11 17:20:00          50      30  19.890000  46.300000  19.200000   \n",
       "2016-01-11 17:30:00          50      40  19.890000  46.066667  19.200000   \n",
       "2016-01-11 17:40:00          60      40  19.890000  46.333333  19.200000   \n",
       "...                         ...     ...        ...        ...        ...   \n",
       "2016-05-27 17:20:00         100       0  25.566667  46.560000  25.890000   \n",
       "2016-05-27 17:30:00          90       0  25.500000  46.500000  25.754000   \n",
       "2016-05-27 17:40:00         270      10  25.500000  46.596667  25.628571   \n",
       "2016-05-27 17:50:00         420      10  25.500000  46.990000  25.414000   \n",
       "2016-05-27 18:00:00         430      10  25.500000  46.600000  25.264286   \n",
       "\n",
       "                          RH_2         T3       RH_3         T4       RH_4  \\\n",
       "date                                                                         \n",
       "2016-01-11 17:00:00  44.790000  19.790000  44.730000  19.000000  45.566667   \n",
       "2016-01-11 17:10:00  44.722500  19.790000  44.790000  19.000000  45.992500   \n",
       "2016-01-11 17:20:00  44.626667  19.790000  44.933333  18.926667  45.890000   \n",
       "2016-01-11 17:30:00  44.590000  19.790000  45.000000  18.890000  45.723333   \n",
       "2016-01-11 17:40:00  44.530000  19.790000  45.000000  18.890000  45.530000   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2016-05-27 17:20:00  42.025714  27.200000  41.163333  24.700000  45.590000   \n",
       "2016-05-27 17:30:00  42.080000  27.133333  41.223333  24.700000  45.590000   \n",
       "2016-05-27 17:40:00  42.768571  27.050000  41.690000  24.700000  45.730000   \n",
       "2016-05-27 17:50:00  43.036000  26.890000  41.290000  24.700000  45.790000   \n",
       "2016-05-27 18:00:00  42.971429  26.823333  41.156667  24.700000  45.963333   \n",
       "\n",
       "                     ...         T9     RH_9      T_out  Press_mm_hg  \\\n",
       "date                 ...                                               \n",
       "2016-01-11 17:00:00  ...  17.033333  45.5300   6.600000        733.5   \n",
       "2016-01-11 17:10:00  ...  17.066667  45.5600   6.483333        733.6   \n",
       "2016-01-11 17:20:00  ...  17.000000  45.5000   6.366667        733.7   \n",
       "2016-01-11 17:30:00  ...  17.000000  45.4000   6.250000        733.8   \n",
       "2016-01-11 17:40:00  ...  17.000000  45.4000   6.133333        733.9   \n",
       "...                  ...        ...      ...        ...          ...   \n",
       "2016-05-27 17:20:00  ...  23.200000  46.7900  22.733333        755.2   \n",
       "2016-05-27 17:30:00  ...  23.200000  46.7900  22.600000        755.2   \n",
       "2016-05-27 17:40:00  ...  23.200000  46.7900  22.466667        755.2   \n",
       "2016-05-27 17:50:00  ...  23.200000  46.8175  22.333333        755.2   \n",
       "2016-05-27 18:00:00  ...  23.200000  46.8450  22.200000        755.2   \n",
       "\n",
       "                        RH_out  Windspeed  Visibility  Tdewpoint        rv1  \\\n",
       "date                                                                          \n",
       "2016-01-11 17:00:00  92.000000   7.000000   63.000000   5.300000  13.275433   \n",
       "2016-01-11 17:10:00  92.000000   6.666667   59.166667   5.200000  18.606195   \n",
       "2016-01-11 17:20:00  92.000000   6.333333   55.333333   5.100000  28.642668   \n",
       "2016-01-11 17:30:00  92.000000   6.000000   51.500000   5.000000  45.410389   \n",
       "2016-01-11 17:40:00  92.000000   5.666667   47.666667   4.900000  10.084097   \n",
       "...                        ...        ...         ...        ...        ...   \n",
       "2016-05-27 17:20:00  55.666667   3.333333   23.666667  13.333333  43.096812   \n",
       "2016-05-27 17:30:00  56.000000   3.500000   24.500000  13.300000  49.282940   \n",
       "2016-05-27 17:40:00  56.333333   3.666667   25.333333  13.266667  29.199117   \n",
       "2016-05-27 17:50:00  56.666667   3.833333   26.166667  13.233333   6.322784   \n",
       "2016-05-27 18:00:00  57.000000   4.000000   27.000000  13.200000  34.118851   \n",
       "\n",
       "                           rv2  \n",
       "date                            \n",
       "2016-01-11 17:00:00  13.275433  \n",
       "2016-01-11 17:10:00  18.606195  \n",
       "2016-01-11 17:20:00  28.642668  \n",
       "2016-01-11 17:30:00  45.410389  \n",
       "2016-01-11 17:40:00  10.084097  \n",
       "...                        ...  \n",
       "2016-05-27 17:20:00  43.096812  \n",
       "2016-05-27 17:30:00  49.282940  \n",
       "2016-05-27 17:40:00  29.199117  \n",
       "2016-05-27 17:50:00   6.322784  \n",
       "2016-05-27 18:00:00  34.118851  \n",
       "\n",
       "[19735 rows x 28 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4a63066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.632289Z",
     "iopub.status.busy": "2023-06-24T17:39:39.631253Z",
     "iopub.status.idle": "2023-06-24T17:39:39.650236Z",
     "shell.execute_reply": "2023-06-24T17:39:39.649211Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.632253Z"
    }
   },
   "outputs": [],
   "source": [
    "df2    = df.copy()\n",
    "scaler = MinMaxScaler().fit(df2)\n",
    "df2    = pd.DataFrame(scaler.transform(df2))\n",
    "\n",
    "X = df2.iloc[:,1:]\n",
    "y = df2.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "673a5f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.653427Z",
     "iopub.status.busy": "2023-06-24T17:39:39.653050Z",
     "iopub.status.idle": "2023-06-24T17:39:39.670480Z",
     "shell.execute_reply": "2023-06-24T17:39:39.669347Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.653384Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train ,X_test ,y_train ,y_test=train_test_split(X ,y ,test_size=0.30,random_state=35)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "X_test  = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92a66651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.672500Z",
     "iopub.status.busy": "2023-06-24T17:39:39.672039Z",
     "iopub.status.idle": "2023-06-24T17:39:39.679174Z",
     "shell.execute_reply": "2023-06-24T17:39:39.678434Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.672460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13814, 27), (2961, 27), (13814,), (2961,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape ,X_test.shape ,y_train.shape ,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e7673b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.680960Z",
     "iopub.status.busy": "2023-06-24T17:39:39.680170Z",
     "iopub.status.idle": "2023-06-24T17:39:39.690552Z",
     "shell.execute_reply": "2023-06-24T17:39:39.689503Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.680931Z"
    }
   },
   "outputs": [],
   "source": [
    "X_val = np.array(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e85592f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.692314Z",
     "iopub.status.busy": "2023-06-24T17:39:39.692008Z",
     "iopub.status.idle": "2023-06-24T17:39:39.702350Z",
     "shell.execute_reply": "2023-06-24T17:39:39.701503Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.692289Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test  = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "X_val = X_val.reshape(X_val.shape[0],1,X_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ede0f7f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.704383Z",
     "iopub.status.busy": "2023-06-24T17:39:39.703777Z",
     "iopub.status.idle": "2023-06-24T17:39:39.718475Z",
     "shell.execute_reply": "2023-06-24T17:39:39.717151Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.704352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13814, 1, 27), (2961, 1, 27))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape ,X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e66895d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:39.721631Z",
     "iopub.status.busy": "2023-06-24T17:39:39.720201Z",
     "iopub.status.idle": "2023-06-24T17:39:41.461519Z",
     "shell.execute_reply": "2023-06-24T17:39:41.460455Z",
     "shell.execute_reply.started": "2023-06-24T17:39:39.721580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 1, 27)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 1, 27)       54          ['input_2[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 1, 27)       1803        ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 1, 27)       0           ['dropout_21[0][0]',             \n",
      " ambda)                                                           'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 1, 32)        0           ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 1, 27)        891         ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 1, 27)       0           ['conv1d_21[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 1, 27)       1803        ['layer_normalization_22[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 1, 27)       0           ['dropout_23[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 1, 32)        0           ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 1, 27)        891         ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 1, 27)       0           ['conv1d_23[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_23[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 1, 27)       1803        ['layer_normalization_24[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 1, 27)       0           ['dropout_25[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_23[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_24[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 1, 32)        0           ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 1, 27)        891         ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 1, 27)       0           ['conv1d_25[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_25[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 1, 27)       1803        ['layer_normalization_26[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 1, 27)       0           ['dropout_27[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_26[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 1, 32)        0           ['conv1d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 1, 27)        891         ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 1, 27)       0           ['conv1d_27[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_27[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 1, 27)       1803        ['layer_normalization_28[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 1, 27)       0           ['dropout_29[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_28[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 1, 32)        0           ['conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 1, 27)        891         ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 1, 27)       0           ['conv1d_29[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_29[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 1, 27)       1803        ['layer_normalization_30[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 1, 27)       0           ['dropout_31[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_30[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 1, 32)        0           ['conv1d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 1, 27)        891         ['dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 1, 27)       0           ['conv1d_31[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_31[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (Multi  (None, 1, 27)       1803        ['layer_normalization_32[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_16[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (TFOpL  (None, 1, 27)       0           ['dropout_33[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_31[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_32[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 1, 32)        0           ['conv1d_32[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 1, 27)        891         ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 1, 27)       0           ['conv1d_33[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_32[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_33[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_17 (Multi  (None, 1, 27)       1803        ['layer_normalization_34[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_17[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (TFOpL  (None, 1, 27)       0           ['dropout_35[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_33[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_34[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 1, 32)        0           ['conv1d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 1, 27)        891         ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 1, 27)       0           ['conv1d_35[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_34[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_36 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_35[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 1, 27)       1803        ['layer_normalization_36[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_18[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 1, 27)       0           ['dropout_37[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_35[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_37 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_36[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 1, 32)        0           ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)             (None, 1, 27)        891         ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 1, 27)       0           ['conv1d_37[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_36[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_38 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_37[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 1, 27)       1803        ['layer_normalization_38[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 1, 27)        0           ['multi_head_attention_19[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 1, 27)       0           ['dropout_39[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_37[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_39 (LayerN  (None, 1, 27)       54          ['tf.__operators__.add_38[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)             (None, 1, 32)        896         ['layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 1, 32)        0           ['conv1d_38[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)             (None, 1, 27)        891         ['dropout_40[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 1, 27)       0           ['conv1d_39[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_38[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 27)          0           ['tf.__operators__.add_39[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          14336       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            513         ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51,829\n",
      "Trainable params: 51,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "model = Transformer(    \n",
    "    input_shape,\n",
    "    n_outputs=1,\n",
    "    head_size=4,\n",
    "    num_heads=4,\n",
    "    ff_dim=32,\n",
    "    num_transformer_blocks=10,\n",
    "    mlp_units=[512],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.2,).build_model()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b83a13ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:39:41.465586Z",
     "iopub.status.busy": "2023-06-24T17:39:41.465206Z",
     "iopub.status.idle": "2023-06-24T17:44:38.029460Z",
     "shell.execute_reply": "2023-06-24T17:44:38.028466Z",
     "shell.execute_reply.started": "2023-06-24T17:39:41.465556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "303/303 [==============================] - 28s 26ms/step - loss: 0.1282 - val_loss: 0.0103\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 7s 23ms/step - loss: 0.0162 - val_loss: 0.0096\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 7s 24ms/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 8s 25ms/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 7s 23ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 7s 22ms/step - loss: 0.0092 - val_loss: 0.0095\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 7s 22ms/step - loss: 0.0092 - val_loss: 0.0094\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 6s 21ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 6s 20ms/step - loss: 0.0091 - val_loss: 0.0095\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0091 - val_loss: 0.0095\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0092 - val_loss: 0.0095\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0091 - val_loss: 0.0094\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0091 - val_loss: 0.0095\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 5s 17ms/step - loss: 0.0091 - val_loss: 0.0094\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0091 - val_loss: 0.0092\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - 5s 17ms/step - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0089 - val_loss: 0.0090\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0088 - val_loss: 0.0092\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0084 - val_loss: 0.0086\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 6s 18ms/step - loss: 0.0082 - val_loss: 0.0086\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 6s 18ms/step - loss: 0.0082 - val_loss: 0.0084\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 6s 19ms/step - loss: 0.0081 - val_loss: 0.0085\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 6s 18ms/step - loss: 0.0081 - val_loss: 0.0085\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 6s 18ms/step - loss: 0.0079 - val_loss: 0.0082\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0080 - val_loss: 0.0082\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0079 - val_loss: 0.0082\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 6s 18ms/step - loss: 0.0079 - val_loss: 0.0085\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0079 - val_loss: 0.0085\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - 5s 17ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 6s 20ms/step - loss: 0.0078 - val_loss: 0.0081\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0076 - val_loss: 0.0082\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0076 - val_loss: 0.0079\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0077 - val_loss: 0.0081\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 5s 17ms/step - loss: 0.0076 - val_loss: 0.0080\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 5s 18ms/step - loss: 0.0075 - val_loss: 0.0082\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 5s 17ms/step - loss: 0.0075 - val_loss: 0.0080\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10979906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:44:38.031707Z",
     "iopub.status.busy": "2023-06-24T17:44:38.030849Z",
     "iopub.status.idle": "2023-06-24T17:44:40.362849Z",
     "shell.execute_reply": "2023-06-24T17:44:40.361846Z",
     "shell.execute_reply.started": "2023-06-24T17:44:38.031669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 2s 5ms/step - loss: 0.0073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0073194000869989395"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98598205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:44:40.364251Z",
     "iopub.status.busy": "2023-06-24T17:44:40.363978Z",
     "iopub.status.idle": "2023-06-24T17:44:40.902548Z",
     "shell.execute_reply": "2023-06-24T17:44:40.901437Z",
     "shell.execute_reply.started": "2023-06-24T17:44:40.364227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 5ms/step - loss: 0.0065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0065373913384974"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cea45daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:44:40.904149Z",
     "iopub.status.busy": "2023-06-24T17:44:40.903861Z",
     "iopub.status.idle": "2023-06-24T17:44:40.911170Z",
     "shell.execute_reply": "2023-06-24T17:44:40.910166Z",
     "shell.execute_reply.started": "2023-06-24T17:44:40.904124Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics(model,Y_train,Y_pred_train,Y_test,Y_pred_test):\n",
    "    print('Train MAE  value   : %.3f ' % mean_absolute_error(Y_train, Y_pred_train))\n",
    "    print('Train MSE  value   : %.3f ' % mean_squared_error(Y_train, Y_pred_train))\n",
    "    print('Train RMSE value   : %.3f ' % root_mean_squared_error(Y_train, Y_pred_train))\n",
    "    print('Train R2   value   : %.3f ' % r2_score(Y_train, Y_pred_train))\n",
    "    print('---------------------------------------------')\n",
    "    print('Test  MAE  value   : %.3f ' % mean_absolute_error(Y_test, Y_pred_test))\n",
    "    print('Test  MSE  value   : %.3f ' % mean_squared_error(Y_test, Y_pred_test))\n",
    "    print('Test  RMSE value   : %.3f ' % root_mean_squared_error(Y_test, Y_pred_test))\n",
    "    print('Test  R2   value   : %.3f ' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66ab067a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:44:40.913584Z",
     "iopub.status.busy": "2023-06-24T17:44:40.912953Z",
     "iopub.status.idle": "2023-06-24T17:44:40.921308Z",
     "shell.execute_reply": "2023-06-24T17:44:40.920381Z",
     "shell.execute_reply.started": "2023-06-24T17:44:40.913558Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error\n",
    "def root_mean_squared_error(y_true, y_pred):    \n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dc25fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:44:40.922943Z",
     "iopub.status.busy": "2023-06-24T17:44:40.922628Z",
     "iopub.status.idle": "2023-06-24T17:44:45.288113Z",
     "shell.execute_reply": "2023-06-24T17:44:45.286925Z",
     "shell.execute_reply.started": "2023-06-24T17:44:40.922918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 4s 5ms/step\n",
      "93/93 [==============================] - 1s 5ms/step\n",
      "--------------------------------------------------\n",
      "Train MAE  value   : 0.042 \n",
      "Train MSE  value   : 0.007 \n",
      "Train RMSE value   : 0.086 \n",
      "Train R2   value   : 0.209 \n",
      "---------------------------------------------\n",
      "Test  MAE  value   : 0.041 \n",
      "Test  MSE  value   : 0.007 \n",
      "Test  RMSE value   : 0.081 \n",
      "Test  R2   value   : 0.226 \n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test  = model.predict(X_test) \n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print_metrics(model , y_train , y_pred_train , y_test , y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "180e5117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-24T17:44:45.289676Z",
     "iopub.status.busy": "2023-06-24T17:44:45.289324Z",
     "iopub.status.idle": "2023-06-24T17:44:45.595966Z",
     "shell.execute_reply": "2023-06-24T17:44:45.595000Z",
     "shell.execute_reply.started": "2023-06-24T17:44:45.289648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# Plot of loss vs epoch for train and test dataset\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Plot of loss vs epoch for train and test dataset\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
